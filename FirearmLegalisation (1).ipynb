{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Research question and objective\n",
        "The user will be prompted to input their research question and objective, from where we would get the necessary keywords and is also used to screen abstracts"
      ],
      "metadata": {
        "id": "SBK07kbERgIR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SEARCH PART\n",
        "\n",
        "Search strategy - Included all relevant papers with keyword extraction, and all the papers are downloaded. Once the papers are retrieved, they go through a screening process using abstracts, rank with respect to relevance."
      ],
      "metadata": {
        "id": "CptZCWFAQLvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJBYeKEM4W0x",
        "outputId": "2c87d327-afc8-4ef3-d0d7-5922326d501f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "E8LcfubX4Z2M",
        "outputId": "303a28ae-e245-4efe-d788-e173e3e8c4f3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-422b2cbd-84a7-46b1-807a-d5bf4d35a2a6\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-422b2cbd-84a7-46b1-807a-d5bf4d35a2a6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Semantic_key.env to Semantic_key.env\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8EE8qum5QJHv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28987403-7ebe-4611-98d2-a9e227643ac6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Welcome to the PRISMA-ScR Automated Research Tool!\n",
            "Enter keywords to search for research papers: What international firearm legislations have been evaluated for their impact on firearm-related suicide, femicide, homicide, and mass shooting rates in Canada and internationally? What has been the impact of Canadian and international firearms legislation on rates of death by firearm-related suicide, femicide, homicide, and mass shootings? What factors have improved or hindered the uptake of Canadian and international firearm legislation?\n",
            "Searching PubMed, DOAJ, SEMANTIC SCHOLAR databases... Please wait...\n",
            "\n",
            "[DOAJ] Response status: 200\n",
            "[Semantic Scholar] Status: 200\n",
            "[Europe PMC] Response status: 200\n",
            "Papers retrieved from each source:\n",
            "Europe PMC: 1 papers\n",
            "DOAJ: 0 papers\n",
            "Semantic Scholar: 0 papers\n",
            "\n",
            "Downloading Paper 1: How firearm legislation impacts firearm mortality internationally: A scoping review.\n",
            "PDF downloaded: downloads/How_firearm_legislation_impacts_firearm_mortality_internationally__A_scoping_review..pdf\n",
            "Search complete!\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import os, time\n",
        "import concurrent.futures\n",
        "from urllib.parse import urlparse\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Function to retreive fulltext url / doi url from DOAJ API response\n",
        "def get_fulltext_from_bibjson(bibjson):\n",
        "    # First try to get fulltext link from DOAJ metadata\n",
        "    for link in bibjson.get(\"link\", []):\n",
        "        if link.get(\"type\") == \"fulltext\":\n",
        "            return link.get(\"url\")\n",
        "\n",
        "    # If full text url not available, build fulltext landing page from DOI (redirects to publisher)\n",
        "    for id_obj in bibjson.get(\"identifier\", []):\n",
        "        if id_obj.get(\"type\") == \"doi\":\n",
        "            return f\"https://doi.org/{id_obj.get('id')}\"\n",
        "\n",
        "    return None\n",
        "\n",
        "# Function to search DOAJ DB\n",
        "def doaj_search(query, page_size=10):\n",
        "    url = f\"https://doaj.org/api/v2/search/articles/{query}?pageSize={page_size}\"\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "    response = requests.get(url, headers=headers)\n",
        "\n",
        "    print(f\"[DOAJ] Response status: {response.status_code}\")\n",
        "    papers = []\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        for result in data.get(\"results\", []):\n",
        "            bibjson = result.get(\"bibjson\", {})\n",
        "            title = bibjson.get(\"title\", \"N/A\")\n",
        "            abstract = bibjson.get(\"abstract\", \"\")\n",
        "            full_text_url = get_fulltext_from_bibjson(bibjson)\n",
        "\n",
        "            papers.append({\n",
        "                \"source\": \"DOAJ\",\n",
        "                \"title\": title,\n",
        "                \"abstract\": abstract,\n",
        "                \"full_text_url\": full_text_url\n",
        "            })\n",
        "    return papers\n",
        "\n",
        "# Function to search EuropePMC DB\n",
        "def europe_pmc_search(query, max_results=30):\n",
        "    params = {\"query\": query + \" + OPEN_ACCESS:Y\", \"format\": \"json\", \"pageSize\": max_results}\n",
        "    url = \"https://www.ebi.ac.uk/europepmc/webservices/rest/search\"\n",
        "    response = requests.get(url, params=params)\n",
        "    print(f\"[Europe PMC] Response status: {response.status_code}\")\n",
        "    papers = []\n",
        "    if response.status_code == 200:\n",
        "        data = response.json().get(\"resultList\", {}).get(\"result\", [])\n",
        "        for item in data:\n",
        "            papers.append({\n",
        "                \"source\": \"Europe PMC\",\n",
        "                \"title\": item.get('title', 'N/A'),\n",
        "                \"abstract\": item.get('abstractText', ''),\n",
        "                \"link\": f\"https://europepmc.org/article/{item.get('source', '')}/{item.get('id', '')}\",\n",
        "                \"full_text_url\": f\"https://europepmc.org/backend/ptpmcrender.fcgi?accid={item.get('pmcid', '')}&blobtype=pdf\"\n",
        "            })\n",
        "    return papers\n",
        "\n",
        "# Function to search semantic scholar DB\n",
        "load_dotenv()\n",
        "def search_semantic_scholar(query, max_results=25):\n",
        "    url = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
        "\n",
        "    # Load the API key from the environment variable\n",
        "    API_KEY = os.getenv(\"API_KEY\")\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {API_KEY}\"\n",
        "    }\n",
        "\n",
        "    params = {\n",
        "        \"query\": query,\n",
        "        \"fields\": \"title,abstract,url,openAccessPdf\",  # Fields you want to retrieve\n",
        "        \"limit\": max_results  # Number of results to return\n",
        "    }\n",
        "\n",
        "    # Retry logic for rate-limiting (HTTP status 429)\n",
        "    retries = 10\n",
        "    for attempt in range(retries):\n",
        "        response = requests.get(url, headers=headers, params=params)\n",
        "\n",
        "        # If the request is successful (status 200), process the response\n",
        "        if response.status_code == 200:\n",
        "            print(f\"[Semantic Scholar] Status: {response.status_code}\")\n",
        "            open_access_papers = []\n",
        "            results = response.json().get(\"data\", [])\n",
        "            for paper in results:\n",
        "                if paper.get(\"openAccessPdf\") and paper[\"openAccessPdf\"].get(\"url\"):\n",
        "                    open_access_papers.append({\n",
        "                        \"source\": \"Semantic Scholar\",\n",
        "                        \"title\": paper.get(\"title\"),\n",
        "                        \"abstract\": paper.get(\"abstract\", \"\"),\n",
        "                        \"full_text_url\": paper[\"openAccessPdf\"][\"url\"]\n",
        "                    })\n",
        "            return open_access_papers\n",
        "\n",
        "        # If rate-limited, back off and retry after a delay\n",
        "        elif response.status_code == 429:\n",
        "            print(f\"[Semantic Scholar] Rate-limited. Retrying in {2 ** attempt} seconds...\")\n",
        "            time.sleep(2 ** attempt)  # Exponential backoff\n",
        "        else:\n",
        "            print(f\"[Semantic Scholar] Error: {response.status_code}\")\n",
        "            break  # Exit on other errors like 4xx, 5xx\n",
        "\n",
        "    # If we exhausted retries and still getting rate-limited\n",
        "    return []\n",
        "\n",
        "\n",
        "# Function to perform parallel search on EuropePMC, DOAJ & Semantic Scholar APIs\n",
        "def parallel_search(query, max_results=30):\n",
        "    results = {\"EUROPEPMC\": [], \"DOAJ\": [], \"SEMANTIC\": []}\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        future_pubmed = executor.submit(europe_pmc_search, query, max_results)\n",
        "        future_doaj = executor.submit(doaj_search, query, max_results)\n",
        "        future_semantic = executor.submit(search_semantic_scholar, query, max_results)\n",
        "\n",
        "        results[\"EUROPEPMC\"] = future_pubmed.result()\n",
        "        results[\"DOAJ\"] = future_doaj.result()\n",
        "        results[\"SEMANTIC\"] = future_semantic.result()\n",
        "\n",
        "    # Display how many papers were retrieved from each API\n",
        "    print(f\"Papers retrieved from each source:\")\n",
        "    print(f\"Europe PMC: {len(results['EUROPEPMC'])} papers\")\n",
        "    print(f\"DOAJ: {len(results['DOAJ'])} papers\")\n",
        "    print(f\"Semantic Scholar: {len(results['SEMANTIC'])} papers\")\n",
        "\n",
        "    # Combine all results and limit the total number of papers\n",
        "    #all_papers = results[\"EUROPEPMC\"] + results[\"DOAJ\"] + results[\"SEMANTIC\"]\n",
        "\n",
        "    # Trim the results to the max_results\n",
        "    return results\n",
        "\n",
        "\n",
        "# Function to check if full text url is a direct pdf link\n",
        "def is_direct_pdf_link(url):\n",
        "    try:\n",
        "        response = requests.head(url, headers={\"User-Agent\": \"Mozilla/5.0\"}, allow_redirects=True, timeout=10)\n",
        "        return 'application/pdf' in response.headers.get(\"Content-Type\", \"\").lower()\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "# Function used to get pdf link from unpaywall if only doi is available\n",
        "def get_pdf_from_unpaywall(doi, email=\"your_email@example.com\"):\n",
        "    api_url = f\"https://api.unpaywall.org/v2/{doi}?email={email}\"\n",
        "    try:\n",
        "        response = requests.get(api_url)\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            oa_location = data.get(\"best_oa_location\")\n",
        "            if oa_location and oa_location.get(\"url_for_pdf\"):\n",
        "                return oa_location[\"url_for_pdf\"]\n",
        "    except:\n",
        "        pass\n",
        "    return None\n",
        "\n",
        "# Function to find pdf link if the full text url is pointing to a webpage instead of direct link\n",
        "def extract_pdf_link_from_html_page(page_url):\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "    try:\n",
        "        response = requests.get(page_url, headers=headers, timeout=10)\n",
        "        if response.status_code != 200:\n",
        "            return None\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        for link in soup.find_all(\"a\", href=True):\n",
        "            classes = link.get(\"class\", [])\n",
        "            href = link[\"href\"]\n",
        "\n",
        "            # Match if any class contains 'pdf' or class list includes 'pdf'\n",
        "            if \"pdf\" in href.lower() or any(\"pdf\" in cls.lower() for cls in classes):\n",
        "                full_link = urljoin(page_url, href)\n",
        "                if full_link.endswith(\".pdf\") or \"view\" in full_link:  # customize as needed\n",
        "                    return full_link\n",
        "    except:\n",
        "        pass\n",
        "    return None\n",
        "\n",
        "# Function to download the pdf to local if the url is a direct link\n",
        "def download_pdf_from_url(pdf_url, save_path=\"paper.pdf\"):\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "    try:\n",
        "        response = requests.get(pdf_url, headers=headers, stream=True, timeout=10)\n",
        "        content_type = response.headers.get(\"Content-Type\", \"\")\n",
        "\n",
        "        if response.status_code == 200 and 'application/pdf' in content_type:\n",
        "            with open(save_path, \"wb\") as f:\n",
        "                for chunk in response.iter_content(1024):\n",
        "                    f.write(chunk)\n",
        "            print(f\"PDF downloaded: {save_path}\")\n",
        "            return save_path\n",
        "        else:\n",
        "            print(f\"Skipped (Not a PDF or blocked): {pdf_url} [Content-Type: {content_type}]\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading PDF from {pdf_url}: {e}\")\n",
        "    return None\n",
        "\n",
        "# Function to rename the downloaded file based on title of the paper\n",
        "def sanitize_filename(title, version=\"\"):\n",
        "    return \"\".join(c if c.isalnum() or c in \"._-\" else \"_\" for c in title)[:100] + (f\"_{version}\" if version else \"\")\n",
        "\n",
        "\n",
        "# Function to download the papers after checking if it is a direct link/doi/webpage\n",
        "def download_all_pdfs(results):\n",
        "    os.makedirs(\"downloads\", exist_ok=True)\n",
        "    #all_papers = results[\"EUROPEPMC\"] + results[\"DOAJ\"] + results[\"SEMANTIC\"]\n",
        "    all_papers = []\n",
        "    for source in [\"EUROPEPMC\", \"DOAJ\", \"SEMANTIC\"]:\n",
        "        all_papers.extend(results[source])\n",
        "\n",
        "    for idx, paper in enumerate(all_papers):\n",
        "        title = paper.get(\"title\", f\"paper_{idx}\")\n",
        "        pdf_url = paper.get(\"full_text_url\")\n",
        "\n",
        "        if pdf_url:\n",
        "            filename = sanitize_filename(title) + \".pdf\"\n",
        "            save_path = os.path.join(\"downloads\", filename)\n",
        "            print(f\"\\nDownloading Paper {idx + 1}: {title}\")\n",
        "            resolve_pdf_url_and_download(pdf_url, save_path)\n",
        "\n",
        "\n",
        "# Function to resolve the pdf url correctly based on api response and download the paper accordingly\n",
        "def resolve_pdf_url_and_download(full_text_url, save_path):\n",
        "\n",
        "    # Direct PDF check\n",
        "    if is_direct_pdf_link(full_text_url):\n",
        "        return download_pdf_from_url(full_text_url, save_path)\n",
        "\n",
        "    # Check if link is DOI and use Unpaywall to download\n",
        "    parsed = urlparse(full_text_url)\n",
        "    if \"doi.org\" in parsed.netloc:\n",
        "        doi = parsed.path.strip(\"/\")\n",
        "        print(f\"Using Unpaywall for DOI: {doi}\")\n",
        "        pdf_url = get_pdf_from_unpaywall(doi)\n",
        "        if pdf_url and is_direct_pdf_link(pdf_url):\n",
        "            return download_pdf_from_url(pdf_url, save_path)\n",
        "\n",
        "    # Scrape the webpage to find PDF url and download\n",
        "    print(f\"Scraping HTML for PDF: {full_text_url}\")\n",
        "    pdf_url = extract_pdf_link_from_html_page(full_text_url)\n",
        "    if pdf_url and is_direct_pdf_link(pdf_url):\n",
        "        return download_pdf_from_url(pdf_url, save_path)\n",
        "\n",
        "    print(f\"No downloadable PDF found\")\n",
        "    return\n",
        "\n",
        "print(\"\\n Welcome to the PRISMA-ScR Automated Research Tool!\")\n",
        "query = input(\"Enter keywords to search for research papers: \")\n",
        "\n",
        "print(\"Searching PubMed, DOAJ, SEMANTIC SCHOLAR databases... Please wait...\\n\")\n",
        "search_results = parallel_search(query, max_results=30)\n",
        "\n",
        "download_all_pdfs(search_results)\n",
        "print(\"Search complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Research question - What international firearm legislations have been evaluated for their impact on firearm-related suicide, femicide, homicide, and mass shooting rates in Canada and internationally? What has been the impact of Canadian and international firearms legislation on rates of death by firearm-related suicide, femicide, homicide, and mass shootings?  What factors have improved or hindered the uptake of Canadian and international firearm legislation?"
      ],
      "metadata": {
        "id": "RZ6ImQx9pk16"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Ensemble Model"
      ],
      "metadata": {
        "id": "qIp9R0ZBbmYj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ensemble model\n",
        "Check the model apis, use the genfuser and pairrank paper to get ideas and come up with prompts"
      ],
      "metadata": {
        "id": "1CcXkQDCXbV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert-score mistralai"
      ],
      "metadata": {
        "id": "__B3W8O2dbr5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e5c279a-40f3-44b2-fb53-41f48ff68dcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bert-score\n",
            "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting mistralai\n",
            "  Downloading mistralai-1.7.0-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.6.0+cu124)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.2.2)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.51.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert-score) (3.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert-score) (24.2)\n",
            "Collecting eval-type-backport>=0.2.0 (from mistralai)\n",
            "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: httpx>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from mistralai) (0.28.1)\n",
            "Requirement already satisfied: pydantic>=2.10.3 in /usr/local/lib/python3.11/dist-packages (from mistralai) (2.11.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from mistralai) (2.9.0.post0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mistralai) (0.4.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->mistralai) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->mistralai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->mistralai) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->mistralai) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.28.1->mistralai) (0.16.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10.3->mistralai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10.3->mistralai) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10.3->mistralai) (4.13.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->mistralai) (1.17.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.0.0->bert-score)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.0.0->bert-score)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.0.0->bert-score)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.0.0->bert-score)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.0.0->bert-score)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.0.0->bert-score)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.0.0->bert-score)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.0.0->bert-score)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.0.0->bert-score)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.0.0->bert-score)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert-score) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.30.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.5.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (3.2.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2.4.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.28.1->mistralai) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.2)\n",
            "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mistralai-1.7.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.5/301.5 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, eval-type-backport, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, mistralai, bert-score\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed bert-score-0.3.13 eval-type-backport-0.2.2 mistralai-1.7.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------FINAL CODE ENSEMBLE-------------------#\n",
        "import numpy as np\n",
        "import json\n",
        "import requests\n",
        "import torch\n",
        "from google import genai\n",
        "from bert_score import score as bert_score\n",
        "from mistralai import Mistral\n",
        "\n",
        "def query_gemini(prompt, api_key):\n",
        "    client = genai.Client(api_key=api_key)\n",
        "    try:\n",
        "        response = client.models.generate_content(\n",
        "          model=\"gemini-2.0-flash\", contents=prompt\n",
        "        )\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        print(f\"Error querying Gemini API: {e}\")\n",
        "        return None\n",
        "\n",
        "def query_mistral(prompt, api_token):\n",
        "    try:\n",
        "        model= \"mistral-small-latest\"\n",
        "        print(\"Sending prompt to Mistral API...\")\n",
        "        client = Mistral(api_key=api_token)\n",
        "        chat_response = client.chat.complete(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": prompt,\n",
        "                }\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        response_content = chat_response.choices[0].message.content\n",
        "\n",
        "        # Try to parse as JSON\n",
        "        try:\n",
        "            json_response = json.loads(response_content)\n",
        "            return json_response\n",
        "        except json.JSONDecodeError:\n",
        "            print(\"Warning: Response is not valid JSON. Returning raw text.\")\n",
        "            return response_content\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error querying Mistral API: {e}\")\n",
        "        return None\n",
        "def parse_ranking_response(response_text):\n",
        "    try:\n",
        "        # Try to extract JSON from the response\n",
        "        start_idx = response_text.find('{')\n",
        "        end_idx = response_text.rfind('}')\n",
        "\n",
        "        if start_idx >= 0 and end_idx > start_idx:\n",
        "            json_str = response_text[start_idx:end_idx+1]\n",
        "            ranking_data = json.loads(json_str)\n",
        "\n",
        "            if isinstance(ranking_data, dict) and \"ranking\" in ranking_data:\n",
        "                return ranking_data[\"ranking\"], ranking_data.get(\"scores\", [])\n",
        "\n",
        "        # Fallback to parsing numbered list\n",
        "        lines = response_text.strip().split('\\n')\n",
        "        ranking = []\n",
        "\n",
        "        for line in lines:\n",
        "            if line and ':' in line:\n",
        "                parts = line.split(':', 1)\n",
        "                try:\n",
        "                    index = int(parts[0].strip().rstrip('.')) - 1\n",
        "                    ranking.append(index)\n",
        "                except ValueError:\n",
        "                    pass\n",
        "\n",
        "        return ranking, [] if ranking else None, []\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing ranking response: {e}\")\n",
        "        return None, []\n",
        "def get_model_rankings(abstracts, titles, query, gemini_api_key, hf_api_token):\n",
        "\n",
        "\n",
        "    abstract_list = \"\"\n",
        "    for i, abs in enumerate(abstracts):\n",
        "        abstract_list += f\"{i+1}. {abs}\\n\"  # If abs is a string. Adjust if dict.\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "      You are a research assistant helping with a scoping review.\n",
        "\n",
        "      RESEARCH QUESTION: {query}\n",
        "\n",
        "      I have retrieved the following {len(abstracts)} papers. Please rank them based on their relevance to the research question:\n",
        "\n",
        "      {abstract_list}\n",
        "\n",
        "      Based on their abstracts, rank these papers in order of relevance to my research question.\n",
        "      For each paper, assign a relevance score from 0 to 10, where 10 is most relevant.\n",
        "\n",
        "      Return your answer as a JSON object with the following format:\n",
        "      {{\n",
        "        \"ranking\": [list of paper numbers in order of relevance from most to least relevant],\n",
        "        \"scores\": [corresponding relevance scores for each paper]\n",
        "      }}\n",
        "\n",
        "      Provide ONLY the JSON response with no additional text.\n",
        "      \"\"\"\n",
        "    gemini_response = query_gemini(prompt, gemini_api_key)\n",
        "    mistral_response = query_mistral(prompt, hf_api_token)\n",
        "    gemini_ranking, gemini_scores = parse_ranking_response(gemini_response) if gemini_response else (None, [])\n",
        "    mistral_ranking, mistral_scores = parse_ranking_response(mistral_response) if mistral_response else (None, [])\n",
        "\n",
        "    results = {\n",
        "        \"gemini\": {\n",
        "            \"ranking\": gemini_ranking if gemini_ranking else [],\n",
        "            \"scores\": gemini_scores if gemini_scores else []\n",
        "        },\n",
        "        \"mistral\": {\n",
        "            \"ranking\": mistral_ranking if mistral_ranking else [],\n",
        "            \"scores\": mistral_scores if mistral_scores else []\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return results\n",
        "\n",
        "def get_ensemble_ranking(abstracts, titles, query, results_from_models=None):\n",
        "\n",
        "    if not abstracts:\n",
        "        return np.arange(len(abstracts)), np.zeros(len(abstracts))\n",
        "\n",
        "    if results_from_models is None:\n",
        "        gemini_api_key = \"AIzaSyDApKHVHxDERiaZDit0Dfpz9XMdQdhL36c\"\n",
        "        hf_api_token = \"Bzj8ue6SoITkGj4hUcRxb9sp56k4aiUa\"\n",
        "        results_from_models = get_model_rankings(abstracts, titles, query, gemini_api_key, hf_api_token)\n",
        "\n",
        "    gemini_ranking = np.array(results_from_models[\"gemini\"][\"ranking\"]) if results_from_models[\"gemini\"][\"ranking\"] else np.arange(len(abstracts))\n",
        "    mistral_ranking = np.array(results_from_models[\"mistral\"][\"ranking\"]) if results_from_models[\"mistral\"][\"ranking\"] else np.arange(len(abstracts))\n",
        "\n",
        "    if len(gemini_ranking) != len(abstracts):\n",
        "        gemini_ranking = np.arange(len(abstracts))\n",
        "    if len(mistral_ranking) != len(abstracts):\n",
        "        mistral_ranking = np.arange(len(abstracts))\n",
        "\n",
        "    ensemble_scores = np.zeros(len(abstracts))\n",
        "    for i in range(len(abstracts)):\n",
        "        gemini_position = np.where(gemini_ranking == i)[0][0] if i in gemini_ranking else len(abstracts)\n",
        "        mistral_position = np.where(mistral_ranking == i)[0][0] if i in mistral_ranking else len(abstracts)\n",
        "\n",
        "        ensemble_scores[i] = 1 / (1 + (gemini_position + mistral_position) / 2)\n",
        "\n",
        "    ensemble_ranked = np.argsort(ensemble_scores)[::-1]\n",
        "\n",
        "    return ensemble_ranked, ensemble_scores\n",
        "\n",
        "def get_model_summaries(abstracts, titles, ranking, query, gemini_api_key, hf_api_token, top_n=5):\n",
        "\n",
        "    top_indices = ranking[:min(top_n, len(ranking))]\n",
        "\n",
        "    papers_info = []\n",
        "    for i, idx in enumerate(top_indices):\n",
        "        paper_info = f\"{i+1}: \\\"{titles[idx]}\\\"\\n\"\n",
        "        paper_info += f\"Abstract: {abstracts[idx]}\\n\"\n",
        "        papers_info.append(paper_info)\n",
        "\n",
        "    papers_text = \"\\n\\n\".join(papers_info)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are a research assistant helping with a scoping review, following PRISMA Guidelines.\n",
        "    RESEARCH QUESTION: {query}\n",
        "    Based on the top {len(top_indices)} papers below, create a comprehensive summary that:\n",
        "    1. Identifies key themes and findings across the papers\n",
        "    2. Highlights methodological approaches used\n",
        "    3. Notes any gaps in the literature\n",
        "    4. Suggests directions for future research\n",
        "    Papers:\n",
        "    {papers_text}\n",
        "    Provide ONLY a well-structured summary that synthesizes the information from these papers with no additional text.\n",
        "    \"\"\"\n",
        "\n",
        "    gemini_summary = query_gemini(prompt, gemini_api_key)\n",
        "    mistral_summary = query_mistral(prompt, hf_api_token)\n",
        "\n",
        "    return {\n",
        "        \"gemini\": gemini_summary,\n",
        "        \"mistral\": mistral_summary\n",
        "    }\n",
        "\n",
        "def get_ensemble_summaries(abstracts, titles, query, results_from_models=None):\n",
        "\n",
        "    if not abstracts:\n",
        "        return \"No abstracts provided for summarization.\"\n",
        "\n",
        "    ensemble_ranked, _ = get_ensemble_ranking(abstracts, titles, query, results_from_models)\n",
        "\n",
        "    gemini_api_key = \"AIzaSyDApKHVHxDERiaZDit0Dfpz9XMdQdhL36c\"\n",
        "    hf_api_token = \"Bzj8ue6SoITkGj4hUcRxb9sp56k4aiUa\"\n",
        "\n",
        "    summaries = get_model_summaries(abstracts, titles, ensemble_ranked, query, gemini_api_key, hf_api_token)\n",
        "\n",
        "    if summaries[\"gemini\"] and summaries[\"mistral\"]:\n",
        "        ensemble_prompt = f\"\"\"\n",
        "        You are a research assistant helping with a scoping review, following PRISMA guidelines.\n",
        "\n",
        "        I have two summaries of the same set of papers related to this research question: \"{query}\"\n",
        "\n",
        "        Summary 1:\n",
        "        {summaries[\"gemini\"]}\n",
        "\n",
        "        Summary 2:\n",
        "        {summaries[\"mistral\"]}\n",
        "\n",
        "        Please create a synthesis of these two summaries, incorporating the strongest insights and analysis from each.\n",
        "        The final summary should be comprehensive yet concise, highlighting key themes, methods, gaps, and future directions.\n",
        "        Do not reference these summaries and just output the final summary.\n",
        "        \"\"\"\n",
        "\n",
        "        ensemble_summary = query_gemini(ensemble_prompt, gemini_api_key)\n",
        "        return ensemble_summary\n",
        "    elif summaries[\"gemini\"]:\n",
        "        return summaries[\"gemini\"]\n",
        "    elif summaries[\"mistral\"]:\n",
        "        return summaries[\"mistral\"]\n",
        "    else:\n",
        "        return \"Unable to generate summaries from the provided models.\"\n",
        "\n",
        "def get_model_summaries_for_each_paper(abstracts, titles, query, gemini_api_key, hf_api_token):\n",
        "\n",
        "    gemini_summary = \"\"\n",
        "    mistral_summary = \"\"\n",
        "    for i in range(len(abstracts)):\n",
        "      prompt = f\"\"\"\n",
        "      Summarize this research paper: Title - {titles[i]}, abstract - {abstracts[i]}\n",
        "      Provide ONLY a well-structured summary that synthesizes the information from these papers with no additional text.\n",
        "      \"\"\"\n",
        "\n",
        "      gemini_summary += (query_gemini(prompt, gemini_api_key))\n",
        "      mistral_summary +=  (query_mistral(prompt, hf_api_token))\n",
        "      time.sleep(1)\n",
        "    return {\n",
        "        \"gemini_ep\": gemini_summary,\n",
        "        \"mistral_ep\": mistral_summary\n",
        "    }\n",
        "\n",
        "def get_ensemble_summaries_for_each_paper(abstracts, titles, query, results_from_models=None):\n",
        "\n",
        "    if not abstracts:\n",
        "        return \"No abstracts provided for summarization.\"\n",
        "    gemini_api_key = \"AIzaSyDApKHVHxDERiaZDit0Dfpz9XMdQdhL36c\"\n",
        "    hf_api_token = \"Bzj8ue6SoITkGj4hUcRxb9sp56k4aiUa\"\n",
        "\n",
        "    summaries = get_model_summaries_for_each_paper(abstracts, titles, query, gemini_api_key, hf_api_token)\n",
        "\n",
        "    if summaries[\"gemini_ep\"] and summaries[\"mistral_ep\"]:\n",
        "        ensemble_prompt = f\"\"\"\n",
        "        I have two summaries of the same set of papers related to this research question: \"{query}\"\n",
        "\n",
        "        Summary 1:\n",
        "        {summaries[\"gemini_ep\"]}\n",
        "\n",
        "        Summary 2:\n",
        "        {summaries[\"mistral_ep\"]}\n",
        "\n",
        "        Please create a synthesis of these two summaries maintaining academic standard. Do not reference these summaries and just output the final summary.\n",
        "        \"\"\"\n",
        "\n",
        "        ensemble_summary = query_gemini(ensemble_prompt, gemini_api_key)\n",
        "        return ensemble_summary\n",
        "    elif summaries[\"gemini_ep\"]:\n",
        "        return summaries[\"gemini_ep\"]\n",
        "    elif summaries[\"mistral_ep\"]:\n",
        "        return summaries[\"mistral_ep\"]\n",
        "    else:\n",
        "        return \"Unable to generate summaries from the provided models.\"\n",
        "\n",
        "def evaluate_summaries_with_bert(summaries, reference_summary):\n",
        "\n",
        "    scores = {}\n",
        "\n",
        "    for model_name, summary in summaries.items():\n",
        "        if summary:\n",
        "            try:\n",
        "                P, R, F1 = bert_score([summary], [reference_summary], lang=\"en\", rescale_with_baseline=True)\n",
        "                scores[model_name] = {\n",
        "                    \"precision\": P.item(),\n",
        "                    \"recall\": R.item(),\n",
        "                    \"f1\": F1.item()\n",
        "                }\n",
        "            except Exception as e:\n",
        "                print(f\"Error computing BERT Score for {model_name}: {e}\")\n",
        "                scores[model_name] = None\n",
        "\n",
        "    # If we have an ensemble summary, evaluate it too\n",
        "    if \"ensemble\" in summaries and summaries[\"ensemble\"]:\n",
        "        try:\n",
        "            P, R, F1 = bert_score([summaries[\"ensemble\"]], [reference_summary], lang=\"en\", rescale_with_baseline=True)\n",
        "            scores[\"ensemble\"] = {\n",
        "                \"precision\": P.item(),\n",
        "                \"recall\": R.item(),\n",
        "                \"f1\": F1.item()\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Error computing BERT Score for ensemble: {e}\")\n",
        "            scores[\"ensemble\"] = None\n",
        "\n",
        "    return scores"
      ],
      "metadata": {
        "id": "uJfGxxvodTpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Abstract Screening\n",
        "\n",
        "The abstracts of the relevant papers go through screening process below. Screening strategy is used by ranking the papers with sentence transformers and ensemble model. All the ranks obtained by each paper will have a final consolidated ranking from the models to minimise the model bias using Reciprocal Rank Fusion\n",
        "\n",
        "### Sentence Transformers used\n",
        "The following pretrained sentence transformers are used:\n",
        "* BM25\n",
        "* SBERT\n",
        "* SPLADE\n",
        "\n",
        "and Ensemble model of Deepseek and Gemini LLMs.\n",
        "\n",
        "The abstracts are ranked on the relevance similarity scores based on mean consolidated embeddings with other abstracts and research question. Say, we have 'N' total papers retrieved and we use the above models(m ∈ [1,4]),\\\n",
        "<center>$R_{abs_i}^{model_m} = SS_{model_m}(abs_i, \\frac{1}{2N}(Σ_{j!=i}abs_j)+rq/2)$</center>, where,\\\n",
        "\n",
        "* R_{abs_i}^{model_m} is the rank of paper i with respect to model m,\n",
        "* SS_{model_m}(a, b) is similarity score with respect to model m between a and b, a and b are two text embedding vectors,\n",
        "* abs_i is abstract embedding vector of paper i,\n",
        "* rq is embedding vector of research question.\n",
        "\n",
        "Now, we have m ranks for each paper, which can possibly include model bias because they are trained over different kinds of data.\n",
        "\n",
        "To reduce this bias, we adopt RRF:\n",
        "<center> $Rank_{abs_i} = Σ_{j=1}^m \\frac{1}{k+R_{abs_i}^{model_j} }$</center>\n",
        "where,\n",
        "\n",
        "\n",
        "* Rank_{abs_i} is the final rank of i^{th} paper,\n",
        "* k is a constant, generally used 60\n",
        "* R_{abs_i}^{model_j} is the rank of i^{th} paper with respect to model j.\n",
        "\n"
      ],
      "metadata": {
        "id": "oe7pUW6WQqOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znQEk7G936w0",
        "outputId": "dbceb7ad-d697-424d-bc67-2b8e488e5b38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.11/dist-packages (1.25.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXTENSIVE ABSTRACT RETRIEVAL"
      ],
      "metadata": {
        "id": "ZlLTbK8g4TmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from pathlib import Path\n",
        "import fitz  # PyMuPDF\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Download NLTK data if needed\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "def get_abstracts_from_papers(results):\n",
        "    \"\"\"\n",
        "    Extract abstracts from the search results.\n",
        "    If abstracts are missing, attempt to extract them from PDFs.\n",
        "\n",
        "    Args:\n",
        "        results: Dictionary with search results from different sources\n",
        "\n",
        "    Returns:\n",
        "        tuple: (abstracts, titles) lists containing all valid papers\n",
        "    \"\"\"\n",
        "    all_papers = results[\"EUROPEPMC\"] + results[\"DOAJ\"] + results[\"SEMANTIC\"]\n",
        "    abstracts = []\n",
        "    titles = []\n",
        "    papers_without_abstracts = []\n",
        "\n",
        "    # First pass: collect available abstracts and identify papers needing extraction\n",
        "    for paper in all_papers:\n",
        "        title = paper.get(\"title\", \"\")\n",
        "        abstract = paper.get(\"abstract\", \"\")\n",
        "\n",
        "        if title and title.strip():  # We require a title\n",
        "            if abstract and abstract.strip():  # Paper already has an abstract\n",
        "                abstracts.append(abstract)\n",
        "                titles.append(title)\n",
        "            else:\n",
        "                # Save papers needing abstract extraction\n",
        "                papers_without_abstracts.append(paper)\n",
        "\n",
        "    # Second pass: extract abstracts from PDFs for papers that need them\n",
        "    if papers_without_abstracts:\n",
        "        pdf_abstracts, pdf_titles = extract_abstracts_from_pdfs('/content/downloads', papers_without_abstracts)\n",
        "        abstracts.extend(pdf_abstracts)\n",
        "        titles.extend(pdf_titles)\n",
        "\n",
        "    return abstracts, titles\n",
        "\n",
        "def extract_abstracts_from_pdfs(folder_path, papers_without_abstracts):\n",
        "    \"\"\"\n",
        "    Extract abstracts from PDF files for papers with missing abstracts.\n",
        "\n",
        "    Args:\n",
        "        folder_path: Path to folder containing PDF files\n",
        "        papers_without_abstracts: List of paper dictionaries with missing abstracts\n",
        "\n",
        "    Returns:\n",
        "        tuple: (extracted_abstracts, corresponding_titles)\n",
        "    \"\"\"\n",
        "    folder = Path(folder_path)\n",
        "    extracted_abstracts = []\n",
        "    corresponding_titles = []\n",
        "\n",
        "    # Create a mapping from normalized titles to papers\n",
        "    title_to_paper = {normalize_title(paper.get(\"title\", \"\")): paper\n",
        "                      for paper in papers_without_abstracts if paper.get(\"title\")}\n",
        "\n",
        "    # Process each PDF file in the folder\n",
        "    pdf_files = list(folder.glob('*.pdf'))\n",
        "    for pdf_path in pdf_files:\n",
        "        try:\n",
        "            # Extract title from PDF for matching\n",
        "            pdf_title = extract_title_from_pdf(pdf_path)\n",
        "\n",
        "            if pdf_title:\n",
        "                normalized_pdf_title = normalize_title(pdf_title)\n",
        "\n",
        "                # Try to match with papers that need abstracts\n",
        "                matched_paper = None\n",
        "\n",
        "                # Try exact title match first\n",
        "                if normalized_pdf_title in title_to_paper:\n",
        "                    matched_paper = title_to_paper[normalized_pdf_title]\n",
        "                else:\n",
        "                    # Try fuzzy matching\n",
        "                    best_match = find_best_title_match(normalized_pdf_title, title_to_paper.keys())\n",
        "                    if best_match:\n",
        "                        matched_paper = title_to_paper[best_match]\n",
        "\n",
        "                # If we found a match, extract the abstract\n",
        "                if matched_paper:\n",
        "                    abstract = extract_abstract_from_pdf(pdf_path)\n",
        "                    if abstract:\n",
        "                        extracted_abstracts.append(abstract)\n",
        "                        corresponding_titles.append(matched_paper.get(\"title\", \"\"))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {pdf_path.name}: {e}\")\n",
        "\n",
        "    return extracted_abstracts, corresponding_titles\n",
        "\n",
        "def normalize_title(title):\n",
        "    \"\"\"Normalize title for comparison by lowercasing and removing extra spaces.\"\"\"\n",
        "    return re.sub(r'\\s+', ' ', title.lower().strip())\n",
        "\n",
        "def find_best_title_match(pdf_title, candidate_titles, threshold=0.7):\n",
        "    \"\"\"Find the best matching title using token-based similarity.\"\"\"\n",
        "    pdf_title_words = set(re.findall(r'\\b\\w+\\b', pdf_title.lower()))\n",
        "\n",
        "    best_match = None\n",
        "    best_score = 0\n",
        "\n",
        "    for candidate in candidate_titles:\n",
        "        candidate_words = set(re.findall(r'\\b\\w+\\b', candidate.lower()))\n",
        "        if pdf_title_words and candidate_words:\n",
        "            # Calculate Jaccard similarity\n",
        "            intersection = len(pdf_title_words.intersection(candidate_words))\n",
        "            union = len(pdf_title_words.union(candidate_words))\n",
        "            score = intersection / union if union > 0 else 0\n",
        "\n",
        "            if score > threshold and score > best_score:\n",
        "                best_score = score\n",
        "                best_match = candidate\n",
        "\n",
        "    return best_match\n",
        "\n",
        "def extract_title_from_pdf(pdf_path):\n",
        "    \"\"\"Extract title from a PDF file.\"\"\"\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "\n",
        "        # Try to get title from metadata\n",
        "        metadata = doc.metadata\n",
        "        if metadata.get(\"title\") and len(metadata.get(\"title\").strip()) > 5:\n",
        "            doc.close()\n",
        "            return metadata.get(\"title\").strip()\n",
        "\n",
        "        # Extract from first page\n",
        "        text = doc[0].get_text()\n",
        "        doc.close()\n",
        "\n",
        "        # Title is usually the first substantial line\n",
        "        lines = [line.strip() for line in text.split('\\n') if len(line.strip()) > 5]\n",
        "        for line in lines:\n",
        "            if 10 <= len(line) <= 200 and not line.lower().startswith(('doi', 'http', 'www')):\n",
        "                return line\n",
        "\n",
        "        return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting title from PDF: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_abstract_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Extract abstract from a PDF file.\n",
        "\n",
        "    Args:\n",
        "        pdf_path: Path to the PDF file\n",
        "\n",
        "    Returns:\n",
        "        str or None: Extracted abstract or None if not found\n",
        "    \"\"\"\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "        text = \"\"\n",
        "\n",
        "        # Get text from first few pages where abstract is likely to be\n",
        "        for page_num in range(min(3, len(doc))):\n",
        "            text += doc[page_num].get_text()\n",
        "        doc.close()\n",
        "\n",
        "        return find_abstract_in_text(text)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting abstract from PDF: {e}\")\n",
        "        return None\n",
        "\n",
        "def find_abstract_in_text(text):\n",
        "    \"\"\"\n",
        "    Find abstract in the PDF text using multiple strategies.\n",
        "\n",
        "    Args:\n",
        "        text: Extracted text from PDF\n",
        "\n",
        "    Returns:\n",
        "        str or None: Extracted abstract or None if not found\n",
        "    \"\"\"\n",
        "    # Method 1: Look for specifically labeled abstract section\n",
        "    abstract_patterns = [\n",
        "        r\"(?i)abstract[\\s]*[:.\\n]+(.*?)(?:[\\n]{2,}|\\b(?:introduction|keywords|key\\s+words)\\b)\",\n",
        "        r\"(?i)ABSTRACT[\\s]*[:.\\n]+(.*?)(?:[\\n]{2,}|\\b(?:introduction|keywords|key\\s+words)\\b)\",\n",
        "        r\"(?i)Abstract[\\s]*[:.\\n]+(.*?)(?:[\\n]{2,}|\\b(?:introduction|keywords|key\\s+words)\\b)\",\n",
        "        r\"(?i)Summary[\\s]*[:.\\n]+(.*?)(?:[\\n]{2,}|\\b(?:introduction|keywords|key\\s+words)\\b)\",\n",
        "        r\"(?i)SUMMARY[\\s]*[:.\\n]+(.*?)(?:[\\n]{2,}|\\b(?:introduction|keywords|key\\s+words)\\b)\"\n",
        "    ]\n",
        "\n",
        "    for pattern in abstract_patterns:\n",
        "        match = re.search(pattern, text, re.DOTALL)\n",
        "        if match:\n",
        "            abstract = match.group(1).strip()\n",
        "            # Clean up the abstract (remove extra whitespaces, line breaks, etc.)\n",
        "            abstract = re.sub(r'\\s+', ' ', abstract)\n",
        "            return abstract\n",
        "\n",
        "    # Method 2: For academic papers, abstract is often the second paragraph after the title\n",
        "    paragraphs = re.split(r'\\n{2,}', text)\n",
        "    if len(paragraphs) > 2:\n",
        "        # If second paragraph is short (likely authors/affiliations), try third paragraph\n",
        "        candidate = paragraphs[1].strip()\n",
        "        if len(candidate.split()) < 30 and len(paragraphs) > 3:\n",
        "            candidate = paragraphs[0].strip()\n",
        "\n",
        "        # Clean up and verify it looks like an abstract\n",
        "        candidate = re.sub(r'\\s+', ' ', candidate)\n",
        "        words = candidate.split()\n",
        "        if 30 < len(words) < 500:  # Typical abstract length\n",
        "            return ' '.join(words)\n",
        "\n",
        "    # Method 3: Look for the first paragraph that looks like an abstract (sentence-based approach)\n",
        "    paragraphs = [re.sub(r'\\s+', ' ', p.strip()) for p in re.split(r'\\n{2,}', text) if p.strip()]\n",
        "    for paragraph in paragraphs[:5]:  # Check first 5 paragraphs\n",
        "        sentences = sent_tokenize(paragraph)\n",
        "        # Abstract usually has multiple sentences and meaningful length\n",
        "        if len(sentences) >= 2 and 50 < len(paragraph) < 2000:\n",
        "            return paragraph\n",
        "\n",
        "    return None"
      ],
      "metadata": {
        "id": "8po-6WpOl98C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rank_bm25"
      ],
      "metadata": {
        "id": "g36geKZ-0vVA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fbb7ac5-5c3d-4111-c03f-e28ce5fa2f2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rank_bm25 in /usr/local/lib/python3.11/dist-packages (0.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rank_bm25) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Reciprocal Rank Fusion (RRF) Function\n",
        "def reciprocal_rank_fusion(ranked_lists, k=60):\n",
        "    scores = {}\n",
        "    for rank_list in ranked_lists:\n",
        "        for rank, doc_id in enumerate(rank_list):\n",
        "            scores[doc_id] = scores.get(doc_id, 0) + 1 / (k + rank + 1)\n",
        "    return sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# BM25 Ranking\n",
        "def get_bm25_ranking(abstracts, query):\n",
        "    tokenized_abstracts = [doc.split() for doc in abstracts]\n",
        "    bm25 = BM25Okapi(tokenized_abstracts)\n",
        "    bm25_scores = bm25.get_scores(query.split())\n",
        "    return np.argsort(bm25_scores)[::-1], bm25_scores\n",
        "\n",
        "# SBERT Ranking\n",
        "def get_sbert_ranking(abstracts, query):\n",
        "    sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    abstract_embeddings = sbert_model.encode(abstracts, convert_to_tensor=True)\n",
        "    query_embedding = sbert_model.encode([query], convert_to_tensor=True)\n",
        "\n",
        "    # Calculate mean abstract embedding to use in similarity calculation\n",
        "    mean_abstract_embedding = torch.mean(abstract_embeddings, dim=0, keepdim=True)\n",
        "    # Combine mean abstract and research question as described in the formula\n",
        "    combined_embedding = 0.5 * mean_abstract_embedding + 0.5 * query_embedding\n",
        "\n",
        "    # Calculate similarity scores for each abstract with the combined embedding\n",
        "    abstract_embeddings = abstract_embeddings.cpu().numpy()\n",
        "    combined_embedding = combined_embedding.cpu().numpy()\n",
        "    sbert_scores = cosine_similarity(abstract_embeddings, combined_embedding).flatten()\n",
        "\n",
        "    return np.argsort(sbert_scores)[::-1], sbert_scores\n",
        "\n",
        "# SPLADE Ranking\n",
        "def get_splade_ranking(abstracts, query):\n",
        "    try:\n",
        "        splade_tokenizer = AutoTokenizer.from_pretrained(\"naver/splade-cocondenser-ensembledistil\")\n",
        "        splade_model = AutoModel.from_pretrained(\"naver/splade-cocondenser-ensembledistil\")\n",
        "\n",
        "        def get_splade_representation(text):\n",
        "            inputs = splade_tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "            with torch.no_grad():\n",
        "                outputs = splade_model(**inputs).last_hidden_state.mean(dim=1)\n",
        "            return outputs.squeeze().cpu().numpy()\n",
        "\n",
        "        splade_embeddings = np.array([get_splade_representation(text) for text in abstracts])\n",
        "        query_splade_embedding = get_splade_representation(query)\n",
        "\n",
        "        # Calculate mean abstract embedding\n",
        "        mean_splade_embedding = np.mean(splade_embeddings, axis=0)\n",
        "        # Combine mean abstract and research question\n",
        "        combined_embedding = 0.5 * mean_splade_embedding + 0.5 * query_splade_embedding\n",
        "\n",
        "        splade_scores = cosine_similarity(splade_embeddings, combined_embedding.reshape(1, -1)).flatten()\n",
        "        return np.argsort(splade_scores)[::-1], splade_scores\n",
        "    except Exception as e:\n",
        "        print(f\"Error in SPLADE ranking: {e}\")\n",
        "        # Return dummy ranking if SPLADE fails\n",
        "        return np.arange(len(abstracts)), np.zeros(len(abstracts))\n",
        "\n",
        "def printRankings(ranked_results):\n",
        "  for i in ranked_results:\n",
        "    print(f'Rank: {i[\"rank\"]}       Title:{i[\"title\"]}       Relevance Score:{i[\"relevance_score\"]}')\n",
        "    print(\"\\n\")\n",
        "\n",
        "def rank_abstracts(search_results, research_question, rfm=None):\n",
        "    # Extract abstracts from search results\n",
        "    abstracts, titles = get_abstracts_from_papers(search_results)\n",
        "\n",
        "    if not abstracts:\n",
        "        print(\"No abstracts found in the search results\")\n",
        "        return []\n",
        "\n",
        "    print(f\"Ranking {len(abstracts)} abstracts based on relevance to research question...\")\n",
        "\n",
        "    # Get rankings from each model\n",
        "    bm25_ranked, bm25_scores = get_bm25_ranking(abstracts, research_question)\n",
        "    sbert_ranked, sbert_scores = get_sbert_ranking(abstracts, research_question)\n",
        "    splade_ranked, splade_scores = get_splade_ranking(abstracts, research_question)\n",
        "\n",
        "    # Collect results from the first three models to use in ensemble\n",
        "    results_from_models = {\n",
        "        \"bm25\": {\"ranking\": bm25_ranked.tolist(), \"scores\": bm25_scores.tolist()},\n",
        "        \"sbert\": {\"ranking\": sbert_ranked.tolist(), \"scores\": sbert_scores.tolist()},\n",
        "        \"splade\": {\"ranking\": splade_ranked.tolist(), \"scores\": splade_scores.tolist()}\n",
        "    }\n",
        "\n",
        "    # Get ensemble model ranking\n",
        "    ensemble_ranked, ensemble_scores = get_ensemble_ranking(abstracts, titles, research_question, rfm)\n",
        "\n",
        "    # Apply RRF to Combine Rankings\n",
        "    ranked_lists = [bm25_ranked, sbert_ranked, splade_ranked, ensemble_ranked]\n",
        "    final_ranking = reciprocal_rank_fusion(ranked_lists)\n",
        "\n",
        "    # Create the final ranked results\n",
        "    ranked_results = []\n",
        "    for idx, (doc_id, score) in enumerate(final_ranking):\n",
        "        if doc_id < len(titles):  # Ensure valid index\n",
        "            ranked_results.append({\n",
        "                \"rank\": idx + 1,\n",
        "                \"title\": titles[doc_id],\n",
        "                \"abstract\": abstracts[doc_id],\n",
        "                \"relevance_score\": score\n",
        "            })\n",
        "\n",
        "    # Save rankings for later use\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "    with open(\"results/abstract_rankings.json\", \"w\") as f:\n",
        "        json.dump(ranked_results, f, indent=2)\n",
        "\n",
        "    return ranked_results"
      ],
      "metadata": {
        "id": "kiO3DObMWgz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ranking the abstracts"
      ],
      "metadata": {
        "id": "W48dAFc8DzCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------CODE TO RUN THE PIPELINE----------------------#\n",
        "def run_review_pipeline(query, abstracts, titles, reference_summary=None):\n",
        "\n",
        "    gemini_api_key = \"AIzaSyDApKHVHxDERiaZDit0Dfpz9XMdQdhL36c\"\n",
        "    hf_api_token = \"Bzj8ue6SoITkGj4hUcRxb9sp56k4aiUa\"\n",
        "\n",
        "    print(\"Getting model rankings...\")\n",
        "    model_rankings = get_model_rankings(abstracts, titles, query, gemini_api_key, hf_api_token)\n",
        "\n",
        "    print(\"Creating ensemble ranking...\")\n",
        "    ensemble_ranked, ensemble_scores = get_ensemble_ranking(abstracts, titles, query, model_rankings)\n",
        "\n",
        "    print(\"Final ranking...\")\n",
        "    final_ranking = rank_abstracts(search_results, query, model_rankings)\n",
        "\n",
        "    print(\"Generating model summaries...\")\n",
        "    model_summaries = get_model_summaries(abstracts, titles, ensemble_ranked, query, gemini_api_key, hf_api_token)\n",
        "\n",
        "    print(\"Creating ensemble summary...\")\n",
        "    ensemble_summary = get_ensemble_summaries(abstracts, titles, query, model_rankings)\n",
        "\n",
        "    print(\"Generating model summaries for each paper...\")\n",
        "    model_summaries_ep = get_model_summaries_for_each_paper(abstracts, titles, query, gemini_api_key, hf_api_token)\n",
        "\n",
        "    print(\"Creating ensemble summary for each paper..\")\n",
        "    ensemble_summary_ep = get_ensemble_summaries_for_each_paper(abstracts, titles, query, model_rankings)\n",
        "\n",
        "    all_summaries = {\n",
        "        \"gemini\": model_summaries[\"gemini\"],\n",
        "        \"mistral\": model_summaries[\"mistral\"],\n",
        "        \"ensemble\": ensemble_summary,\n",
        "        \"gemini_ep\": model_summaries_ep[\"gemini_ep\"],\n",
        "        \"mistral_ep\": model_summaries_ep[\"mistral_ep\"],\n",
        "        \"ensemble_ep\": ensemble_summary_ep\n",
        "    }\n",
        "\n",
        "    results = {\n",
        "        \"rankings\": {\n",
        "            \"gemini\": model_rankings[\"gemini\"],\n",
        "            \"mistral\": model_rankings[\"mistral\"],\n",
        "            \"ensemble\": {\n",
        "                \"ranking\": ensemble_ranked.tolist(),\n",
        "                \"scores\": ensemble_scores.tolist()\n",
        "            },\n",
        "            \"final\" : final_ranking\n",
        "        },\n",
        "        \"summaries\": all_summaries\n",
        "    }\n",
        "\n",
        "    # Step 5: Evaluate with BERT Score if reference is provided\n",
        "    if reference_summary:\n",
        "        print(\"Evaluating summaries with BERT Score...\")\n",
        "        bert_scores = evaluate_summaries_with_bert(all_summaries, reference_summary)\n",
        "        results[\"evaluation\"] = bert_scores\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "ayTLrQUYlPGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ref_summary = \"\"\"\n",
        "Background\n",
        "The literature on gun violence is broad and variable, describing multiple legislation types and outcomes in observational studies. Our objective was to document the extent and nature of evidence on the impact of firearm legislation on mortality from firearm violence.\n",
        "Methods\n",
        "A scoping review was conducted under PRISMA-ScR guidance. A comprehensive peer-reviewed search strategy was executed in several electronic databases from inception to March 2024. Grey literature was searched for unpublished sources. Data were extracted on study design, country, population, type of legislation, and overall study conclusions on legislation impact on mortality from suicide, homicide, femicide, and domestic violence. Critical appraisal for a sample of articles with the same study design (ecological studies) was conducted for quality assessment.\n",
        "Findings\n",
        "5057 titles and abstracts and 651 full-text articles were reviewed. Following full-text review and grey literature search, 202 articles satisfied our eligibility criteria. Federal legislation was identified from all included countries, while state-specific laws were only reported in studies from the U.S. Numerous legislative approaches were identified including preventative, prohibitive, and more tailored strategies focused on identifying high risk individuals. Law types had various effects on rates of firearm homicide, suicide, and femicide. Lack of robust design, uneven implementation, and poor evaluation of legislation may contribute to these differences.\n",
        "Interpretation\n",
        "We found that national, restrictive laws reduce population-level firearm mortality. These findings can inform policy makers, public health researchers, and governments when designing and implementing legislation to reduce injury and death from firearms.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "eX8gE_rdeXE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abstracts, titles = get_abstracts_from_papers(search_results)\n",
        "results = run_review_pipeline(query, abstracts, titles, ref_summary)\n",
        "\n",
        "# Print rankings\n",
        "print(\"\\nPaper Rankings:\")\n",
        "print(\"==============\")\n",
        "print(\"\\nGemini Ranking:\")\n",
        "# print(\"results!!!----------->\", results)\n",
        "for i, idx in enumerate(results[\"rankings\"][\"gemini\"][\"ranking\"]):\n",
        "    print(f\"{i+1}. {titles[idx-1]}\")\n",
        "\n",
        "print(\"\\nMistral Ranking:\")\n",
        "for i, idx in enumerate(results[\"rankings\"][\"mistral\"][\"ranking\"]):\n",
        "    print(f\"{i+1}. {titles[idx-1]}\")\n",
        "\n",
        "print(\"\\nEnsemble Ranking:\")\n",
        "for i, idx in enumerate(results[\"rankings\"][\"ensemble\"][\"ranking\"]):\n",
        "    print(f\"{i+1}. {titles[idx]} (Score: {results['rankings']['ensemble']['scores'][i]:.4f})\")\n",
        "\n",
        "print(\"\\nFinal Ranking:\")\n",
        "for i in (results[\"rankings\"][\"final\"]):\n",
        "  print(f\"{i['rank']}.{i['title']} (Score: {i['relevance_score']:.4f})\")\n",
        "\n",
        "# Print summaries (truncated for brevity)\n",
        "print(\"\\nConsolidated Summary:\")\n",
        "print(\"=================================\")\n",
        "for model, summary in results[\"summaries\"].items():\n",
        "    if summary:\n",
        "        print(f\"\\n{model.capitalize()} Summary: {summary[:]}...\")\n",
        "\n",
        "print(\"\\nSummaries for each paper:\")\n",
        "print(\"=================================\")\n",
        "\n",
        "# Print BERT Score evaluation\n",
        "if \"evaluation\" in results:\n",
        "    print(\"\\nBERT Score Evaluation:\")\n",
        "    print(\"=====================\")\n",
        "    for model, scores in results[\"evaluation\"].items():\n",
        "        if scores:\n",
        "            print(f\"\\n{model.capitalize()}:\")\n",
        "            print(f\"  Precision: {scores['precision']:.4f}\")\n",
        "            print(f\"  Recall: {scores['recall']:.4f}\")\n",
        "            print(f\"  F1: {scores['f1']:.4f}\")\n"
      ],
      "metadata": {
        "id": "5qGgP4xmlf5A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a986fa08-5c01-4059-e6e4-1d8de288240f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error extracting abstract from PDF: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('punkt_tab')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n",
            "Getting model rankings...\n",
            "Sending prompt to Mistral API...\n",
            "Warning: Response is not valid JSON. Returning raw text.\n",
            "Creating ensemble ranking...\n",
            "Final ranking...\n",
            "Error extracting abstract from PDF: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('punkt_tab')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n",
            "No abstracts found in the search results\n",
            "Generating model summaries...\n",
            "Sending prompt to Mistral API...\n",
            "Warning: Response is not valid JSON. Returning raw text.\n",
            "Creating ensemble summary...\n",
            "Generating model summaries for each paper...\n",
            "Creating ensemble summary for each paper..\n",
            "Evaluating summaries with BERT Score...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Paper Rankings:\n",
            "==============\n",
            "\n",
            "Gemini Ranking:\n",
            "\n",
            "Mistral Ranking:\n",
            "\n",
            "Ensemble Ranking:\n",
            "\n",
            "Final Ranking:\n",
            "\n",
            "Consolidated Summary:\n",
            "=================================\n",
            "\n",
            "Gemini Summary: ## Summary of Literature on Firearm Legislation and Firearm-Related Violence\n",
            "\n",
            "**1. Key Themes and Findings:**\n",
            "\n",
            "*   **Varied Legislative Approaches:** Studies evaluate a range of firearm laws, including restrictions on firearm ownership (e.g., background checks, waiting periods, permit-to-purchase laws), restrictions on specific types of firearms (e.g., assault weapons bans), and safe storage laws.\n",
            "*   **Inconsistent Impact:** The impact of firearm legislation on firearm-related violence outcomes (suicide, homicide, mass shootings) varies across studies and jurisdictions. Some studies find evidence of a decrease in firearm violence following the implementation of specific laws, while others find no significant effect or even an increase.\n",
            "*   **Context Matters:** The effectiveness of firearm legislation is influenced by contextual factors such as pre-existing levels of firearm violence, the specific characteristics of the legislation, the degree of enforcement, and the presence of other concurrent policies.\n",
            "*   **Focus on Homicide and Suicide:** A large portion of studies focuses on the impact of legislation on firearm-related homicide and suicide rates. Femicide and mass shootings are less frequently examined.\n",
            "*   **Mixed Results for Specific Laws:**\n",
            "    *   **Background checks/Permit-to-purchase:** Some studies suggest a potential association with reduced firearm violence, while others do not find a significant effect.\n",
            "    *   **Assault weapon bans:** Evidence for the effectiveness of these bans is mixed, with some studies showing a reduction in mass shooting fatalities and others showing no significant impact.\n",
            "*   **Femicide:** few papers study the impact of firearm legislation on femicide.\n",
            "\n",
            "**2. Methodological Approaches:**\n",
            "\n",
            "*   **Ecological Studies:** Many studies use ecological designs, comparing firearm violence rates across different jurisdictions with varying firearm laws or examining changes in firearm violence rates within a jurisdiction before and after the implementation of new laws.\n",
            "*   **Time-Series Analysis:** Time-series analysis is used to assess the trends in firearm violence rates over time and to determine if there is a significant change in these trends following the implementation of firearm legislation.\n",
            "*   **Cross-Sectional Studies:** Cross-sectional studies compare firearm violence rates across different jurisdictions at a single point in time.\n",
            "*   **Regression Analysis:** Regression models are often used to control for confounding variables and to isolate the effect of firearm legislation on firearm violence rates.\n",
            "*   **Systematic Reviews/Meta-Analyses:** These studies synthesize the findings of multiple individual studies to provide a more comprehensive assessment of the evidence.\n",
            "\n",
            "**3. Gaps in the Literature:**\n",
            "\n",
            "*   **Limited Focus on Femicide:** The impact of firearm legislation on femicide is understudied, and more research is needed to understand the relationship between firearm access and violence against women.\n",
            "*   **Mass Shooting Research:** Research on the impact of firearm legislation on mass shootings is limited due to the rarity of these events and the challenges in isolating the effect of specific laws.\n",
            "*   **Implementation and Enforcement:** Studies often fail to adequately address the implementation and enforcement of firearm laws, which can significantly impact their effectiveness.\n",
            "*   **Qualitative Data:** Studies often lack qualitative components exploring the lived experiences or perceptions of firearm legislation uptake, impact, or hindrance factors.\n",
            "*   **Lack of Specificity:** Legislation is often studied broadly, without consideration for nuances within particular laws which hinders clear interpreation of impact.\n",
            "\n",
            "**4. Directions for Future Research:**\n",
            "\n",
            "*   **Femicide Studies:** Conduct more research on the impact of firearm legislation on femicide, including studies that examine the role of firearms in domestic violence fatalities.\n",
            "*   **Mass Shooting Analysis:** Utilize innovative research methods to study the impact of firearm legislation on mass shootings, such as case studies or simulations.\n",
            "*   **Implementation Studies:** Conduct studies that examine the implementation and enforcement of firearm laws, including studies that assess the impact of different enforcement strategies.\n",
            "*   **Qualitative Studies:** Conduct qualitative studies to understand the perspectives of stakeholders (e.g., law enforcement, gun owners, community members) on firearm legislation and its impact.\n",
            "*   **Longitudinal Studies:** Conduct long-term longitudinal studies to assess the sustained impact of firearm legislation on firearm violence rates.\n",
            "*   **International Comparisons:** Conduct more cross-national studies to compare the impact of firearm legislation across different countries and to identify factors that contribute to the effectiveness of firearm laws.\n",
            "*   **Mixed-methods Studies:** Combining quantitative analysis with qualitative insights will provide a more comprehensive understanding of the complex relationship between firearm legislation and firearm violence.\n",
            "*   **Study Nuances of Laws:** Conduct studies analyzing specific aspects of laws, rather than broad legislation.\n",
            "...\n",
            "\n",
            "Mistral Summary: ### Comprehensive Summary\n",
            "\n",
            "#### Key Themes and Findings\n",
            "\n",
            "1. **Impact of Firearm Legislation on Suicide Rates**:\n",
            "   - Studies consistently show that stricter firearm legislation is associated with reduced firearm-related suicide rates. Countries with more stringent gun control laws, such as Australia and Canada, have reported significant declines in firearm suicides post-legislation.\n",
            "   - The availability and accessibility of firearms are critical factors in suicide rates, with restrictions on firearm ownership leading to a shift towards other methods of suicide, but overall suicide rates often do not increase.\n",
            "\n",
            "2. **Impact on Femicide and Homicide Rates**:\n",
            "   - Legislation aimed at reducing domestic violence and femicide has shown mixed results. Some studies indicate a decrease in firearm-related femicides, while others suggest that non-firearm femicides may increase.\n",
            "   - Homicide rates, particularly those involving firearms, tend to decrease in regions with stricter gun control laws. However, the effectiveness varies based on the specific legislation and enforcement mechanisms.\n",
            "\n",
            "3. **Mass Shootings**:\n",
            "   - There is evidence that comprehensive firearm legislation can reduce the frequency and lethality of mass shootings. Countries with stricter gun laws, such as Australia, have seen a marked reduction in mass shooting incidents post-legislation.\n",
            "   - Background checks, waiting periods, and restrictions on high-capacity magazines are identified as effective measures in reducing mass shooting incidents.\n",
            "\n",
            "4. **Factors Influencing Legislation Uptake**:\n",
            "   - Political climate and public opinion significantly influence the passage and enforcement of firearm legislation. Strong public support and political will are crucial for successful implementation.\n",
            "   - Economic factors, such as the cost of compliance and enforcement, also play a role in the uptake of legislation. Countries with robust economic resources tend to have more effective and comprehensive firearm laws.\n",
            "   - Cultural attitudes towards firearms and gun ownership vary widely, impacting the acceptance and enforcement of legislation.\n",
            "\n",
            "#### Methodological Approaches\n",
            "\n",
            "- **Quantitative Studies**: Many studies utilized longitudinal data to analyze trends in firearm-related deaths before and after the implementation of legislation. Regression analysis and time-series models were commonly employed to assess the impact of specific laws.\n",
            "- **Qualitative Studies**: Interviews and focus groups with policymakers, law enforcement, and community members provided insights into the factors influencing the uptake and effectiveness of firearm legislation.\n",
            "- **Mixed-Methods Approaches**: Some studies combined quantitative data analysis with qualitative insights to provide a comprehensive understanding of the impact and implementation challenges of firearm legislation.\n",
            "\n",
            "#### Gaps in the Literature\n",
            "\n",
            "- **Longitudinal Studies**: There is a need for more long-term studies that track the impact of firearm legislation over extended periods to account for delayed effects and potential shifts in societal attitudes.\n",
            "- **Cross-Cultural Comparisons**: Few studies have conducted in-depth cross-cultural comparisons, which could provide valuable insights into the effectiveness of different legislative approaches in varying cultural contexts.\n",
            "- **Enforcement and Compliance**: Limited research exists on the enforcement mechanisms and compliance rates of firearm legislation, which are crucial for understanding the real-world impact of these laws.\n",
            "- **Intersectional Analysis**: There is a lack of studies examining the intersectional impacts of firearm legislation on marginalized communities, such as racial and ethnic minorities, LGBTQ+ individuals, and low-income populations.\n",
            "\n",
            "#### Directions for Future Research\n",
            "\n",
            "- **Longitudinal and Cross-Cultural Studies**: Future research should focus on longitudinal studies and cross-cultural comparisons to better understand the long-term and contextual impacts of firearm legislation.\n",
            "- **Enforcement and Compliance**: Studies should explore the effectiveness of enforcement mechanisms and compliance rates to identify best practices for implementation.\n",
            "- **Intersectional Impacts**: Research should delve into the intersectional impacts of firearm legislation to ensure that policies are equitable and effective for all segments of the population.\n",
            "- **Public Health Perspectives**: Incorporating public health frameworks can provide a more holistic view of the impact of firearm legislation on community health and well-being.\n",
            "- **Policy Evaluation**: Future studies should evaluate the effectiveness of specific policy components, such as background checks, waiting periods, and restrictions on certain types of firearms, to identify the most impactful measures....\n",
            "\n",
            "Ensemble Summary: No abstracts provided for summarization....\n",
            "\n",
            "Ensemble_ep Summary: No abstracts provided for summarization....\n",
            "\n",
            "Summaries for each paper:\n",
            "=================================\n",
            "\n",
            "BERT Score Evaluation:\n",
            "=====================\n",
            "\n",
            "Gemini:\n",
            "  Precision: -0.1508\n",
            "  Recall: 0.0267\n",
            "  F1: -0.0619\n",
            "\n",
            "Mistral:\n",
            "  Precision: -0.0547\n",
            "  Recall: 0.0240\n",
            "  F1: -0.0140\n",
            "\n",
            "Ensemble:\n",
            "  Precision: 0.1011\n",
            "  Recall: -0.3380\n",
            "  F1: -0.1266\n",
            "\n",
            "Ensemble_ep:\n",
            "  Precision: 0.1011\n",
            "  Recall: -0.3380\n",
            "  F1: -0.1266\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reference_summary = \"\"\"\n",
        "# Background\n",
        "# Disasters are becoming more frequent due to the impact of extreme weather events attributed to climate change, causing loss of lives, property, and psychological trauma. Mental health response to disasters emphasizes prevention and mitigation, and mobile health (mHealth) apps have been used for mental health promotion and treatment. However, little is known about their use in the mental health components of disaster management.\n",
        "\n",
        "# Objective\n",
        "# This scoping review was conducted to explore the use of mobile phone apps for mental health responses to natural disasters and to identify gaps in the literature.\n",
        "\n",
        "# Methods\n",
        "# We identified relevant keywords and subject headings and conducted comprehensive searches in 6 electronic databases. Studies in which participants were exposed to a man-made disaster were included if the sample also included some participants exposed to a natural hazard. Only full-text studies published in English were included. The initial titles and abstracts of the unique papers were screened by 2 independent review authors. Full texts of the selected papers that met the inclusion criteria were reviewed by the 2 independent reviewers. Data were extracted from each selected full-text paper and synthesized using a narrative approach based on the outcome measures, duration, frequency of use of the mobile phone apps, and the outcomes. This scoping review was reported according to the PRISMA-ScR (Preferred Reporting Items for Systematic Reviews and Meta-Analyses extension for Scoping Reviews).\n",
        "\n",
        "# Results\n",
        "# Of the 1398 papers retrieved, 5 were included in this review. A total of 3 studies were conducted on participants exposed to psychological stress following a disaster while 2 were for disaster relief workers. The mobile phone apps for the interventions included Training for Life Skills, Sonoma Rises, Headspace, Psychological First Aid, and Substance Abuse and Mental Health Services Administration (SAMHSA) Behavioural Health Disaster Response Apps. The different studies assessed the effectiveness or efficacy of the mobile app, feasibility, acceptability, and characteristics of app use or predictors of use. Different measures were used to assess the effectiveness of the apps’ use as either the primary or secondary outcome.\n",
        "\n",
        "# Conclusions\n",
        "# A limited number of studies are exploring the use of mobile phone apps for mental health responses to disasters. The 5 studies included in this review showed promising results. Mobile apps have the potential to provide effective mental health support before, during, and after disasters. However, further research is needed to explore the potential of mobile phone apps in mental health responses to all hazards.\n",
        "\n",
        "# Keywords: mental health, disasters, mobile health, mHealth, application, applications, app, apps, smartphone, stress, psychological, traumatic, disaster, disasters, hazard, hazards, emergency, psychological trauma, mobile apps, trauma, scoping, review methods, review methodology, mobile phone\n",
        "# \"\"\"\n",
        "# abstracts, titles = get_abstracts_from_papers(search_results)\n",
        "# results = run_review_pipeline(query, abstracts, titles, reference_summary)\n",
        "\n",
        "# # Print rankings\n",
        "# print(\"\\nPaper Rankings:\")\n",
        "# print(\"==============\")\n",
        "# print(\"\\nGemini Ranking:\")\n",
        "# # print(\"results!!!----------->\", results)\n",
        "# for i, idx in enumerate(results[\"rankings\"][\"gemini\"][\"ranking\"]):\n",
        "#     print(f\"{i+1}. {titles[idx-1]}\")\n",
        "\n",
        "# print(\"\\nMistral Ranking:\")\n",
        "# for i, idx in enumerate(results[\"rankings\"][\"mistral\"][\"ranking\"]):\n",
        "#     print(f\"{i+1}. {titles[idx-1]}\")\n",
        "\n",
        "# print(\"\\nEnsemble Ranking:\")\n",
        "# for i, idx in enumerate(results[\"rankings\"][\"ensemble\"][\"ranking\"]):\n",
        "#     print(f\"{i+1}. {titles[idx]} (Score: {results['rankings']['ensemble']['scores'][i]:.4f})\")\n",
        "\n",
        "# print(\"\\nFinal Ranking:\")\n",
        "# for i in (results[\"rankings\"][\"final\"]):\n",
        "#   print(f\"{i['rank']}.{i['title']} (Score: {i['relevance_score']:.4f})\")\n",
        "\n",
        "# # Print summaries (truncated for brevity)\n",
        "# print(\"\\nConsolidated Summary:\")\n",
        "# print(\"=================================\")\n",
        "# for model, summary in results[\"summaries\"].items():\n",
        "#     if summary:\n",
        "#         print(f\"\\n{model.capitalize()} Summary: {summary[:]}...\")\n",
        "\n",
        "# # Print BERT Score evaluation\n",
        "# if \"evaluation\" in results:\n",
        "#     print(\"\\nBERT Score Evaluation:\")\n",
        "#     print(\"=====================\")\n",
        "#     for model, scores in results[\"evaluation\"].items():\n",
        "#         if scores:\n",
        "#             print(f\"\\n{model.capitalize()}:\")\n",
        "#             print(f\"  Precision: {scores['precision']:.4f}\")\n",
        "#             print(f\"  Recall: {scores['recall']:.4f}\")\n",
        "#             print(f\"  F1: {scores['f1']:.4f}\")\n"
      ],
      "metadata": {
        "id": "DEL_VI6N7rqp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}