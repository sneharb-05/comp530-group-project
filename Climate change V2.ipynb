{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Research question and objective\n",
        "The user will be prompted to input their research question and objective, from where we would get the necessary keywords and is also used to screen abstracts"
      ],
      "metadata": {
        "id": "SBK07kbERgIR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SEARCH PART\n",
        "\n",
        "Search strategy - Included all relevant papers with keyword extraction, and all the papers are downloaded. Once the papers are retrieved, they go through a screening process using abstracts, rank with respect to relevance."
      ],
      "metadata": {
        "id": "CptZCWFAQLvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJBYeKEM4W0x",
        "outputId": "96743f8f-0a2e-4288-b22e-ca2453c29fb1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "E8LcfubX4Z2M",
        "outputId": "29454164-de87-4d56-ca40-529633ba66fa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-723e6951-aea5-4071-9892-b5d33cd4c9a9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-723e6951-aea5-4071-9892-b5d33cd4c9a9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Semantic_key.env to Semantic_key (1).env\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8EE8qum5QJHv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4eb0981-f27b-4a72-c006-e6d6619142dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Welcome to the PRISMA-ScR Automated Research Tool!\n",
            "Enter keywords to search for research papers: climate change adaptation\n",
            "Searching PubMed, DOAJ, SEMANTIC SCHOLAR databases... Please wait...\n",
            "\n",
            "[DOAJ] Response status: 200\n",
            "[Europe PMC] Response status: 200\n",
            "[Semantic Scholar] Status: 200\n",
            "Papers retrieved from each source:\n",
            "Europe PMC: 30 papers\n",
            "DOAJ: 30 papers\n",
            "Semantic Scholar: 22 papers\n",
            "\n",
            "Downloading Paper 1: Outpacing climate change: adaptation to heatwaves in Europe.\n",
            "PDF downloaded: downloads/Outpacing_climate_change__adaptation_to_heatwaves_in_Europe..pdf\n",
            "\n",
            "Downloading Paper 2: What is limiting how we imagine climate change adaptation?\n",
            "Scraping HTML for PDF: https://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC11625676&blobtype=pdf\n",
            "No downloadable PDF found\n",
            "\n",
            "Downloading Paper 3: OLDER ADULTS AND CLIMATE CHANGE ADAPTATION STRATEGIES: A SCOPING REVIEW\n",
            "PDF downloaded: downloads/OLDER_ADULTS_AND_CLIMATE_CHANGE_ADAPTATION_STRATEGIES__A_SCOPING_REVIEW.pdf\n",
            "\n",
            "Downloading Paper 4: Flavonoids and anthocyanins in seagrasses: implications for climate change adaptation and resilience.\n",
            "PDF downloaded: downloads/Flavonoids_and_anthocyanins_in_seagrasses__implications_for_climate_change_adaptation_and_resilience.pdf\n",
            "\n",
            "Downloading Paper 5: Socio-economic factors constrain climate change adaptation in a tropical export crop.\n",
            "PDF downloaded: downloads/Socio-economic_factors_constrain_climate_change_adaptation_in_a_tropical_export_crop..pdf\n",
            "\n",
            "Downloading Paper 6: Reinforcement learning-based adaptive strategies for climate change adaptation: An application for coastal flood risk management.\n",
            "PDF downloaded: downloads/Reinforcement_learning-based_adaptive_strategies_for_climate_change_adaptation__An_application_for_c.pdf\n",
            "\n",
            "Downloading Paper 7: Indigenous knowledge and leadership for climate change adaptation in nutrition.\n",
            "PDF downloaded: downloads/Indigenous_knowledge_and_leadership_for_climate_change_adaptation_in_nutrition..pdf\n",
            "\n",
            "Downloading Paper 8: Thermal Plasticity and Evolutionary Constraints in <i>Bacillus</i>: Implications for Climate Change Adaptation.\n",
            "PDF downloaded: downloads/Thermal_Plasticity_and_Evolutionary_Constraints_in__i_Bacillus__i___Implications_for_Climate_Change_.pdf\n",
            "\n",
            "Downloading Paper 9: Integrating disaster risk reduction and climate change adaptation in Seychelles: Challenges and proposed strategies.\n",
            "PDF downloaded: downloads/Integrating_disaster_risk_reduction_and_climate_change_adaptation_in_Seychelles__Challenges_and_prop.pdf\n",
            "\n",
            "Downloading Paper 10: Constraining the entire Earth system projections for more reliable climate change adaptation planning.\n",
            "PDF downloaded: downloads/Constraining_the_entire_Earth_system_projections_for_more_reliable_climate_change_adaptation_plannin.pdf\n",
            "\n",
            "Downloading Paper 11: Climate change adaptation strategies adopted by pastoralists in rangelands in Golestan province, Iran.\n",
            "PDF downloaded: downloads/Climate_change_adaptation_strategies_adopted_by_pastoralists_in_rangelands_in_Golestan_province__Ira.pdf\n",
            "\n",
            "Downloading Paper 12: Neo-tropical species production: a sustainable strategy for climate change adaptation in neo-tropical regions.\n",
            "PDF downloaded: downloads/Neo-tropical_species_production__a_sustainable_strategy_for_climate_change_adaptation_in_neo-tropica.pdf\n",
            "\n",
            "Downloading Paper 13: Climate change adaptation measures in the context of health promotion using participatory foresight\n",
            "PDF downloaded: downloads/Climate_change_adaptation_measures_in_the_context_of_health_promotion_using_participatory_foresight.pdf\n",
            "\n",
            "Downloading Paper 14: Incremental and transformational climate change adaptation factors in agriculture worldwide: A comparative analysis using natural language processing.\n",
            "PDF downloaded: downloads/Incremental_and_transformational_climate_change_adaptation_factors_in_agriculture_worldwide__A_compa.pdf\n",
            "\n",
            "Downloading Paper 15: Scoping Review of Climate Change Adaptation Interventions for Health: Implications for Policy and Practice.\n",
            "PDF downloaded: downloads/Scoping_Review_of_Climate_Change_Adaptation_Interventions_for_Health__Implications_for_Policy_and_Pr.pdf\n",
            "\n",
            "Downloading Paper 16: Digital twins for managing bridge climate change adaptation.\n",
            "PDF downloaded: downloads/Digital_twins_for_managing_bridge_climate_change_adaptation..pdf\n",
            "\n",
            "Downloading Paper 17: Soft computing paradigm for climate change adaptation and mitigation in Iran, Pakistan, and Turkey: A systematic review.\n",
            "PDF downloaded: downloads/Soft_computing_paradigm_for_climate_change_adaptation_and_mitigation_in_Iran__Pakistan__and_Turkey__.pdf\n",
            "\n",
            "Downloading Paper 18: A relational turn in climate change adaptation: Evidence from urban nature-based solutions.\n",
            "PDF downloaded: downloads/A_relational_turn_in_climate_change_adaptation__Evidence_from_urban_nature-based_solutions..pdf\n",
            "\n",
            "Downloading Paper 19: Analysis of drought and extreme precipitation events in Thailand: trends, climate modeling, and implications for climate change adaptation.\n",
            "PDF downloaded: downloads/Analysis_of_drought_and_extreme_precipitation_events_in_Thailand__trends__climate_modeling__and_impl.pdf\n",
            "\n",
            "Downloading Paper 20: The role of indigenous knowledge in disaster risk reduction and climate change adaptation in Chikwawa, Malawi.\n",
            "PDF downloaded: downloads/The_role_of_indigenous_knowledge_in_disaster_risk_reduction_and_climate_change_adaptation_in_Chikwaw.pdf\n",
            "\n",
            "Downloading Paper 21: Climate change adaptation must not replicate lockdown scenarios.\n",
            "PDF downloaded: downloads/Climate_change_adaptation_must_not_replicate_lockdown_scenarios..pdf\n",
            "\n",
            "Downloading Paper 22: The Role of Climate Change Adaptation in Enhancing Household Food Security: A Case Study of the Hamassa Watershed Agroecologies, Southern Ethiopia.\n",
            "PDF downloaded: downloads/The_Role_of_Climate_Change_Adaptation_in_Enhancing_Household_Food_Security__A_Case_Study_of_the_Hama.pdf\n",
            "\n",
            "Downloading Paper 23: Uncovering the reasons behind the failure of pastoralists in adopting climate change adaptation strategies.\n",
            "PDF downloaded: downloads/Uncovering_the_reasons_behind_the_failure_of_pastoralists_in_adopting_climate_change_adaptation_stra.pdf\n",
            "\n",
            "Downloading Paper 24: African Local Pig Genetic Resources in the Context of Climate Change Adaptation.\n",
            "PDF downloaded: downloads/African_Local_Pig_Genetic_Resources_in_the_Context_of_Climate_Change_Adaptation..pdf\n",
            "\n",
            "Downloading Paper 25: Health systems response to climate change adaptation: a scoping review of global evidence.\n",
            "PDF downloaded: downloads/Health_systems_response_to_climate_change_adaptation__a_scoping_review_of_global_evidence..pdf\n",
            "\n",
            "Downloading Paper 26: Assessing alternative lake management actions for climate change adaptation.\n",
            "PDF downloaded: downloads/Assessing_alternative_lake_management_actions_for_climate_change_adaptation..pdf\n",
            "\n",
            "Downloading Paper 27: Integrating science and the arts to deglobalise climate change adaptation.\n",
            "PDF downloaded: downloads/Integrating_science_and_the_arts_to_deglobalise_climate_change_adaptation..pdf\n",
            "\n",
            "Downloading Paper 28: The role of the public health service in the implementation of heat health action plans for climate change adaptation in Germany: A qualitative study.\n",
            "PDF downloaded: downloads/The_role_of_the_public_health_service_in_the_implementation_of_heat_health_action_plans_for_climate_.pdf\n",
            "\n",
            "Downloading Paper 29: Understanding How Indigenous Knowledge Contributes to Climate Change Adaptation and Resilience: A Systematic Literature Review.\n",
            "PDF downloaded: downloads/Understanding_How_Indigenous_Knowledge_Contributes_to_Climate_Change_Adaptation_and_Resilience__A_Sy.pdf\n",
            "\n",
            "Downloading Paper 30: Nature-based solutions in spatial planning and policies for climate change adaptation: A literature review.\n",
            "PDF downloaded: downloads/Nature-based_solutions_in_spatial_planning_and_policies_for_climate_change_adaptation__A_literature_.pdf\n",
            "\n",
            "Downloading Paper 31: Évaluer ex ante la pertinence de projets locaux d’adaptation au changement climatique\n",
            "Scraping HTML for PDF: https://journals.openedition.org/vertigo/13000\n",
            "No downloadable PDF found\n",
            "\n",
            "Downloading Paper 32: Non-state actor perceptions of legitimacy and meaningful participation in international climate governance\n",
            "Using Unpaywall for DOI: 10.1038/s44168-025-00214-9\n",
            "Scraping HTML for PDF: https://doi.org/10.1038/s44168-025-00214-9\n",
            "No downloadable PDF found\n",
            "\n",
            "Downloading Paper 33: Integrative Approaches to Soybean Resilience, Productivity, and Utility: A Review of Genomics, Computational Modeling, and Economic Viability\n",
            "Scraping HTML for PDF: https://www.mdpi.com/2223-7747/14/5/671\n",
            "No downloadable PDF found\n",
            "\n",
            "Downloading Paper 34: Impacts of land-use changes and landholding fragmentation on crop water demand and drought in Wadi El-Farigh, New Delta project, Egypt\n",
            "Scraping HTML for PDF: http://www.sciencedirect.com/science/article/pii/S1110982322000734\n",
            "No downloadable PDF found\n",
            "\n",
            "Downloading Paper 35: AGRICULTURE IN THE FACE OF CLIMATE CHALLENGES – THE PROBLEM OF GREENHOUSE GAS EMISSIONS\n",
            "Scraping HTML for PDF: http://rnseria.com/gicid/01.3001.0013.2178\n",
            "No downloadable PDF found\n",
            "\n",
            "Downloading Paper 36: Climate change and agricultural ecosystem: Challenges and microbial interventions for mitigation\n",
            "Scraping HTML for PDF: https://journal.agrimetassociation.org/index.php/jam/article/view/2305\n",
            "No downloadable PDF found\n",
            "\n",
            "Downloading Paper 37: Spatial risk assessment for climate proofing of economic activities: The case of Belluno Province (North-East Italy)\n",
            "Scraping HTML for PDF: http://www.sciencedirect.com/science/article/pii/S2212096324000731\n",
            "No downloadable PDF found\n",
            "\n",
            "Downloading Paper 38: High Coral Recruitment Despite Coralline Algal Loss Under Extreme Environmental Conditions\n",
            "Scraping HTML for PDF: https://www.frontiersin.org/articles/10.3389/fmars.2022.837877/full\n",
            "No downloadable PDF found\n",
            "\n",
            "Downloading Paper 39: Managing Climate Change: The Role of Multi-Stakeholder Partnerships in Building Climate Resilience in Sub-Saharan Africa\n",
            "Scraping HTML for PDF: https://pubs.lib.umn.edu/index.php/ijps/article/view/3386\n",
            "No downloadable PDF found\n",
            "\n",
            "Downloading Paper 40: Participatory implementation within climate change related policies in urbanized area of Indonesia\n",
            "Scraping HTML for PDF: https://jurnal.ugm.ac.id/ijg/article/view/36263\n",
            "No downloadable PDF found\n",
            "\n",
            "Downloading Paper 41: Justice in Urban Climate Change Adaptation: Criteria and Application to Delhi\n",
            "Scraping HTML for PDF: http://www.ecologyandsociety.org/vol18/iss4/art48/\n",
            "PDF downloaded: downloads/Justice_in_Urban_Climate_Change_Adaptation__Criteria_and_Application_to_Delhi.pdf\n",
            "\n",
            "Downloading Paper 42: Thermal regime and host clade, rather than geography, drive Symbiodinium and bacterial assemblages in the scleractinian coral Pocillopora damicornis sensu lato\n",
            "Scraping HTML for PDF: http://link.springer.com/article/10.1186/s40168-018-0423-6\n",
            "PDF downloaded: downloads/Thermal_regime_and_host_clade__rather_than_geography__drive_Symbiodinium_and_bacterial_assemblages_i.pdf\n",
            "\n",
            "Downloading Paper 43: Salinity Constraints for Small-Scale Agriculture and Impact on Adaptation in North Aceh, Indonesia\n",
            "Scraping HTML for PDF: https://www.mdpi.com/2073-4395/12/2/341\n",
            "No downloadable PDF found\n",
            "\n",
            "Downloading Paper 44: A large set of potential past, present and future hydro-meteorological time series for the UK\n",
            "PDF downloaded: downloads/A_large_set_of_potential_past__present_and_future_hydro-meteorological_time_series_for_the_UK.pdf\n",
            "\n",
            "Downloading Paper 45: Physiological Adaptation to Water Salinity in Six Wild Halophytes Suitable for Mediterranean Agriculture\n",
            "Scraping HTML for PDF: https://www.mdpi.com/2223-7747/10/2/309\n",
            "No downloadable PDF found\n",
            "\n",
            "Downloading Paper 46: Variations of Near Surface Energy Balance Caused by Land Cover Changes in the Semiarid Grassland Area of China\n",
            "Using Unpaywall for DOI: 10.1155/2014/894147\n",
            "Scraping HTML for PDF: http://dx.doi.org/10.1155/2014/894147\n",
            "No downloadable PDF found\n",
            "\n",
            "Downloading Paper 47: Climate variability, trends, projections and their impact on different crops:  A case study of Gujarat, India\n",
            "Scraping HTML for PDF: https://journal.agrimetassociation.org/index.php/jam/article/view/2151\n",
            "No downloadable PDF found\n",
            "\n",
            "Downloading Paper 48: Factors influencing livestock ownership and herd intensity among smallholder farmers in the Eastern Cape, South Africa\n",
            "Scraping HTML for PDF: http://www.sciencedirect.com/science/article/pii/S2405844025001677\n",
            "No downloadable PDF found\n",
            "\n",
            "Downloading Paper 49: Student-Created Videos of Climate Change Vulnerability: Opportunity for connection and care\n",
            "Scraping HTML for PDF: https://www.ajol.info/index.php/sajee/article/view/238387\n",
            "PDF downloaded: downloads/Student-Created_Videos_of_Climate_Change_Vulnerability__Opportunity_for_connection_and_care.pdf\n",
            "\n",
            "Downloading Paper 50: Climate change and aquaculture: considering biological response and resources\n",
            "Scraping HTML for PDF: https://www.int-res.com/abstracts/aei/v11/p569-602/\n",
            "No downloadable PDF found\n",
            "\n",
            "Downloading Paper 51: Solar Panels reduce both global warming and Urban Heat Island\n",
            "Scraping HTML for PDF: http://journal.frontiersin.org/Journal/10.3389/fenvs.2014.00014/full\n",
            "No downloadable PDF found\n",
            "\n",
            "Downloading Paper 52: Estimating climate-induced ‘Nowhere to go’ range shifts of the Himalayan Incarvillea Juss. using multi-model median ensemble species distribution models\n",
            "Scraping HTML for PDF: http://www.sciencedirect.com/science/article/pii/S1470160X20310669\n",
            "No downloadable PDF found\n",
            "\n",
            "Downloading Paper 53: Coastal flooding and mean sea-level rise allowances in atoll island\n",
            "Using Unpaywall for DOI: 10.1038/s41598-022-05329-1\n",
            "Scraping HTML for PDF: https://doi.org/10.1038/s41598-022-05329-1\n",
            "No downloadable PDF found\n",
            "\n",
            "Downloading Paper 54: Research for climate adaptation\n",
            "Using Unpaywall for DOI: 10.1038/s43247-021-00294-5\n",
            "Scraping HTML for PDF: https://doi.org/10.1038/s43247-021-00294-5\n",
            "No downloadable PDF found\n",
            "\n",
            "Downloading Paper 55: Paradiplomatie et finance climatique mondiale : Les cas du Québec, de la Wallonie et de l’Écosse\n",
            "Scraping HTML for PDF: https://journals.openedition.org/vertigo/39720\n",
            "No downloadable PDF found\n",
            "\n",
            "Downloading Paper 56: Characterising the short- and long-term impacts of tropical cyclones on mangroves using the Landsat archive\n",
            "Scraping HTML for PDF: https://www.cambridge.org/core/product/identifier/S2754720524000192/type/journal_article\n",
            "PDF downloaded: downloads/Characterising_the_short-_and_long-term_impacts_of_tropical_cyclones_on_mangroves_using_the_Landsat_.pdf\n",
            "\n",
            "Downloading Paper 57: Economic impacts of climate change on agriculture: a comparison of process-based and statistical yield models\n",
            "Using Unpaywall for DOI: 10.1088/1748-9326/aa6eb2\n",
            "Scraping HTML for PDF: https://doi.org/10.1088/1748-9326/aa6eb2\n",
            "No downloadable PDF found\n",
            "\n",
            "Downloading Paper 58: A Framework Towards Resource Integration and Energy Efficiency Auditing with Zero Hazardous Discharge Programme\n",
            "Scraping HTML for PDF: https://www.cetjournal.it/index.php/cet/article/view/12307\n",
            "PDF downloaded: downloads/A_Framework_Towards_Resource_Integration_and_Energy_Efficiency_Auditing_with_Zero_Hazardous_Discharg.pdf\n",
            "\n",
            "Downloading Paper 59: “ADAPTING TO A CHANGING WORLD: STRATEGIES FOR MITIGATING THE GLOBAL CONSEQUENCES OF CLIMATE CHANGE”\n",
            "Scraping HTML for PDF: https://wissjournals.com.ng/index.php/wiss/article/view/478\n",
            "No downloadable PDF found\n",
            "\n",
            "Downloading Paper 60: Understanding the links between human health, ecosystem health, and food systems in Small Island Developing States using stakeholder-informed causal loop diagrams.\n",
            "Using Unpaywall for DOI: 10.1371/journal.pgph.0001988\n",
            "Scraping HTML for PDF: https://doi.org/10.1371/journal.pgph.0001988\n",
            "PDF downloaded: downloads/Understanding_the_links_between_human_health__ecosystem_health__and_food_systems_in_Small_Island_Dev.pdf\n",
            "\n",
            "Downloading Paper 61: Climate Change 2022 – Impacts, Adaptation and Vulnerability\n",
            "PDF downloaded: downloads/Climate_Change_2022___Impacts__Adaptation_and_Vulnerability.pdf\n",
            "\n",
            "Downloading Paper 62: Climate Change 2021 – The Physical Science Basis\n",
            "Scraping HTML for PDF: https://www.cambridge.org/core/services/aop-cambridge-core/content/view/84D59430721AC15204CEAFA4F3902A42/stamped-9781009157889pre1_i-ii.pdf/frontmatter.pdf\n",
            "No downloadable PDF found\n",
            "\n",
            "Downloading Paper 63: Climate Change 2014 - Synthesis Report\n",
            "PDF downloaded: downloads/Climate_Change_2014_-_Synthesis_Report.pdf\n",
            "\n",
            "Downloading Paper 64: Adaptation of Agriculture to Climate Change: A Scoping Review\n",
            "Scraping HTML for PDF: https://www.mdpi.com/2225-1154/11/10/202/pdf?version=1696644434\n",
            "No downloadable PDF found\n",
            "\n",
            "Downloading Paper 65: A review of the global climate change impacts, adaptation, and sustainable mitigation measures\n",
            "Scraping HTML for PDF: https://link.springer.com/content/pdf/10.1007/s11356-022-19718-6.pdf\n",
            "No downloadable PDF found\n",
            "\n",
            "Downloading Paper 66: Effective climate change adaptation means supporting community autonomy\n",
            "PDF downloaded: downloads/Effective_climate_change_adaptation_means_supporting_community_autonomy.pdf\n",
            "\n",
            "Downloading Paper 67: Mapping the effectiveness of nature‐based solutions for climate change adaptation\n",
            "Scraping HTML for PDF: https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/gcb.15310\n",
            "No downloadable PDF found\n",
            "\n",
            "Downloading Paper 68: Meta-analyses of factors motivating climate change adaptation behaviour\n",
            "Scraping HTML for PDF: https://research.rug.nl/files/74065148/vanValkengoedSteg2019_manuscript_authors_copy.pdf\n",
            "No downloadable PDF found\n",
            "\n",
            "Downloading Paper 69: Deploying artificial intelligence for climate change adaptation\n",
            "PDF downloaded: downloads/Deploying_artificial_intelligence_for_climate_change_adaptation.pdf\n",
            "\n",
            "Downloading Paper 70: Can public awareness, knowledge and engagement improve climate change adaptation policies?\n",
            "PDF downloaded: downloads/Can_public_awareness__knowledge_and_engagement_improve_climate_change_adaptation_policies_.pdf\n",
            "\n",
            "Downloading Paper 71: Indigenous knowledge on climate change adaptation: a global evidence map of academic literature\n",
            "Using Unpaywall for DOI: 10.1088/1748-9326/abb330\n",
            "Scraping HTML for PDF: https://doi.org/10.1088/1748-9326/abb330\n",
            "No downloadable PDF found\n",
            "\n",
            "Downloading Paper 72: Public participation, engagement, and climate change adaptation: A review of the research literature\n",
            "Scraping HTML for PDF: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9285715\n",
            "No downloadable PDF found\n",
            "\n",
            "Downloading Paper 73: Climate change adaptation in aquaculture\n",
            "PDF downloaded: downloads/Climate_change_adaptation_in_aquaculture.pdf\n",
            "\n",
            "Downloading Paper 74: Global hunger and climate change adaptation through international trade\n",
            "PDF downloaded: downloads/Global_hunger_and_climate_change_adaptation_through_international_trade.pdf\n",
            "\n",
            "Downloading Paper 75: Climate change adaptation strategies, productivity and sustainable food security in southern Mali\n",
            "Using Unpaywall for DOI: 10.1007/s10584-020-02684-8\n",
            "Scraping HTML for PDF: https://doi.org/10.1007/s10584-020-02684-8\n",
            "No downloadable PDF found\n",
            "\n",
            "Downloading Paper 76: Climate change adaptation in SIDS: A systematic review of the literature pre and post the IPCC Fifth Assessment Report\n",
            "Scraping HTML for PDF: https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/wcc.653\n",
            "No downloadable PDF found\n",
            "\n",
            "Downloading Paper 77: A framework to diagnose barriers to climate change adaptation\n",
            "PDF downloaded: downloads/A_framework_to_diagnose_barriers_to_climate_change_adaptation.pdf\n",
            "\n",
            "Downloading Paper 78: Nature-Based Solutions for Urban Climate Change Adaptation: Linking Science, Policy, and Practice Communities for Evidence-Based Decision-Making\n",
            "Scraping HTML for PDF: https://academic.oup.com/bioscience/article-pdf/69/6/455/32900702/biz042.pdf\n",
            "No downloadable PDF found\n",
            "\n",
            "Downloading Paper 79: Interrogating ‘effectiveness’ in climate change adaptation: 11 guiding principles for adaptation research and practice\n",
            "Scraping HTML for PDF: https://www.tandfonline.com/doi/pdf/10.1080/17565529.2021.1964937?needAccess=true\n",
            "No downloadable PDF found\n",
            "\n",
            "Downloading Paper 80: Beyond participation: when citizen engagement leads to undesirable outcomes for nature-based solutions and climate change adaptation\n",
            "PDF downloaded: downloads/Beyond_participation__when_citizen_engagement_leads_to_undesirable_outcomes_for_nature-based_solutio.pdf\n",
            "\n",
            "Downloading Paper 81: A systematic global stocktake of evidence on human adaptation to climate change\n",
            "Scraping HTML for PDF: https://vtechworks.lib.vt.edu/bitstreams/79d478df-7412-4710-b92a-984bec9f670a/download\n",
            "No downloadable PDF found\n",
            "\n",
            "Downloading Paper 82: Climate change adaptation: a study of multiple climate-smart practices in the Nile Basin of Ethiopia\n",
            "Scraping HTML for PDF: https://www.tandfonline.com/doi/pdf/10.1080/17565529.2018.1442801?needAccess=true\n",
            "No downloadable PDF found\n",
            "Search complete!\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import os, time\n",
        "import concurrent.futures\n",
        "from urllib.parse import urlparse\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Function to retreive fulltext url / doi url from DOAJ API response\n",
        "def get_fulltext_from_bibjson(bibjson):\n",
        "    # First try to get fulltext link from DOAJ metadata\n",
        "    for link in bibjson.get(\"link\", []):\n",
        "        if link.get(\"type\") == \"fulltext\":\n",
        "            return link.get(\"url\")\n",
        "\n",
        "    # If full text url not available, build fulltext landing page from DOI (redirects to publisher)\n",
        "    for id_obj in bibjson.get(\"identifier\", []):\n",
        "        if id_obj.get(\"type\") == \"doi\":\n",
        "            return f\"https://doi.org/{id_obj.get('id')}\"\n",
        "\n",
        "    return None\n",
        "\n",
        "# Function to search DOAJ DB\n",
        "def doaj_search(query, page_size=10):\n",
        "    url = f\"https://doaj.org/api/v2/search/articles/{query}?pageSize={page_size}\"\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "    response = requests.get(url, headers=headers)\n",
        "\n",
        "    print(f\"[DOAJ] Response status: {response.status_code}\")\n",
        "    papers = []\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        for result in data.get(\"results\", []):\n",
        "            bibjson = result.get(\"bibjson\", {})\n",
        "            title = bibjson.get(\"title\", \"N/A\")\n",
        "            abstract = bibjson.get(\"abstract\", \"\")\n",
        "            full_text_url = get_fulltext_from_bibjson(bibjson)\n",
        "\n",
        "            papers.append({\n",
        "                \"source\": \"DOAJ\",\n",
        "                \"title\": title,\n",
        "                \"abstract\": abstract,\n",
        "                \"full_text_url\": full_text_url\n",
        "            })\n",
        "    return papers\n",
        "\n",
        "# Function to search EuropePMC DB\n",
        "def europe_pmc_search(query, max_results=30):\n",
        "    params = {\"query\": query + \" + OPEN_ACCESS:Y\", \"format\": \"json\", \"pageSize\": max_results}\n",
        "    url = \"https://www.ebi.ac.uk/europepmc/webservices/rest/search\"\n",
        "    response = requests.get(url, params=params)\n",
        "    print(f\"[Europe PMC] Response status: {response.status_code}\")\n",
        "    papers = []\n",
        "    if response.status_code == 200:\n",
        "        data = response.json().get(\"resultList\", {}).get(\"result\", [])\n",
        "        for item in data:\n",
        "            papers.append({\n",
        "                \"source\": \"Europe PMC\",\n",
        "                \"title\": item.get('title', 'N/A'),\n",
        "                \"abstract\": item.get('abstractText', ''),\n",
        "                \"link\": f\"https://europepmc.org/article/{item.get('source', '')}/{item.get('id', '')}\",\n",
        "                \"full_text_url\": f\"https://europepmc.org/backend/ptpmcrender.fcgi?accid={item.get('pmcid', '')}&blobtype=pdf\"\n",
        "            })\n",
        "    return papers\n",
        "\n",
        "# Function to search semantic scholar DB\n",
        "load_dotenv()\n",
        "def search_semantic_scholar(query, max_results=25):\n",
        "    url = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
        "\n",
        "    # Load the API key from the environment variable\n",
        "    API_KEY = os.getenv(\"API_KEY\")\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {API_KEY}\"\n",
        "    }\n",
        "\n",
        "    params = {\n",
        "        \"query\": query,\n",
        "        \"fields\": \"title,abstract,url,openAccessPdf\",  # Fields you want to retrieve\n",
        "        \"limit\": max_results  # Number of results to return\n",
        "    }\n",
        "\n",
        "    # Retry logic for rate-limiting (HTTP status 429)\n",
        "    retries = 10\n",
        "    for attempt in range(retries):\n",
        "        response = requests.get(url, headers=headers, params=params)\n",
        "\n",
        "        # If the request is successful (status 200), process the response\n",
        "        if response.status_code == 200:\n",
        "            print(f\"[Semantic Scholar] Status: {response.status_code}\")\n",
        "            open_access_papers = []\n",
        "            results = response.json().get(\"data\", [])\n",
        "            for paper in results:\n",
        "                if paper.get(\"openAccessPdf\") and paper[\"openAccessPdf\"].get(\"url\"):\n",
        "                    open_access_papers.append({\n",
        "                        \"source\": \"Semantic Scholar\",\n",
        "                        \"title\": paper.get(\"title\"),\n",
        "                        \"abstract\": paper.get(\"abstract\", \"\"),\n",
        "                        \"full_text_url\": paper[\"openAccessPdf\"][\"url\"]\n",
        "                    })\n",
        "            return open_access_papers\n",
        "\n",
        "        # If rate-limited, back off and retry after a delay\n",
        "        elif response.status_code == 429:\n",
        "            print(f\"[Semantic Scholar] Rate-limited. Retrying in {2 ** attempt} seconds...\")\n",
        "            time.sleep(2 ** attempt)  # Exponential backoff\n",
        "        else:\n",
        "            print(f\"[Semantic Scholar] Error: {response.status_code}\")\n",
        "            break  # Exit on other errors like 4xx, 5xx\n",
        "\n",
        "    # If we exhausted retries and still getting rate-limited\n",
        "    return []\n",
        "\n",
        "\n",
        "# Function to perform parallel search on EuropePMC, DOAJ & Semantic Scholar APIs\n",
        "def parallel_search(query, max_results=30):\n",
        "    results = {\"EUROPEPMC\": [], \"DOAJ\": [], \"SEMANTIC\": []}\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        future_pubmed = executor.submit(europe_pmc_search, query, max_results)\n",
        "        future_doaj = executor.submit(doaj_search, query, max_results)\n",
        "        future_semantic = executor.submit(search_semantic_scholar, query, max_results)\n",
        "\n",
        "        results[\"EUROPEPMC\"] = future_pubmed.result()\n",
        "        results[\"DOAJ\"] = future_doaj.result()\n",
        "        results[\"SEMANTIC\"] = future_semantic.result()\n",
        "\n",
        "    # Display how many papers were retrieved from each API\n",
        "    print(f\"Papers retrieved from each source:\")\n",
        "    print(f\"Europe PMC: {len(results['EUROPEPMC'])} papers\")\n",
        "    print(f\"DOAJ: {len(results['DOAJ'])} papers\")\n",
        "    print(f\"Semantic Scholar: {len(results['SEMANTIC'])} papers\")\n",
        "\n",
        "    # Combine all results and limit the total number of papers\n",
        "    #all_papers = results[\"EUROPEPMC\"] + results[\"DOAJ\"] + results[\"SEMANTIC\"]\n",
        "\n",
        "    # Trim the results to the max_results\n",
        "    return results\n",
        "\n",
        "\n",
        "# Function to check if full text url is a direct pdf link\n",
        "def is_direct_pdf_link(url):\n",
        "    try:\n",
        "        response = requests.head(url, headers={\"User-Agent\": \"Mozilla/5.0\"}, allow_redirects=True, timeout=10)\n",
        "        return 'application/pdf' in response.headers.get(\"Content-Type\", \"\").lower()\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "# Function used to get pdf link from unpaywall if only doi is available\n",
        "def get_pdf_from_unpaywall(doi, email=\"your_email@example.com\"):\n",
        "    api_url = f\"https://api.unpaywall.org/v2/{doi}?email={email}\"\n",
        "    try:\n",
        "        response = requests.get(api_url)\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            oa_location = data.get(\"best_oa_location\")\n",
        "            if oa_location and oa_location.get(\"url_for_pdf\"):\n",
        "                return oa_location[\"url_for_pdf\"]\n",
        "    except:\n",
        "        pass\n",
        "    return None\n",
        "\n",
        "# Function to find pdf link if the full text url is pointing to a webpage instead of direct link\n",
        "def extract_pdf_link_from_html_page(page_url):\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "    try:\n",
        "        response = requests.get(page_url, headers=headers, timeout=10)\n",
        "        if response.status_code != 200:\n",
        "            return None\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        for link in soup.find_all(\"a\", href=True):\n",
        "            classes = link.get(\"class\", [])\n",
        "            href = link[\"href\"]\n",
        "\n",
        "            # Match if any class contains 'pdf' or class list includes 'pdf'\n",
        "            if \"pdf\" in href.lower() or any(\"pdf\" in cls.lower() for cls in classes):\n",
        "                full_link = urljoin(page_url, href)\n",
        "                if full_link.endswith(\".pdf\") or \"view\" in full_link:  # customize as needed\n",
        "                    return full_link\n",
        "    except:\n",
        "        pass\n",
        "    return None\n",
        "\n",
        "# Function to download the pdf to local if the url is a direct link\n",
        "def download_pdf_from_url(pdf_url, save_path=\"paper.pdf\"):\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "    try:\n",
        "        response = requests.get(pdf_url, headers=headers, stream=True, timeout=10)\n",
        "        content_type = response.headers.get(\"Content-Type\", \"\")\n",
        "\n",
        "        if response.status_code == 200 and 'application/pdf' in content_type:\n",
        "            with open(save_path, \"wb\") as f:\n",
        "                for chunk in response.iter_content(1024):\n",
        "                    f.write(chunk)\n",
        "            print(f\"PDF downloaded: {save_path}\")\n",
        "            return save_path\n",
        "        else:\n",
        "            print(f\"Skipped (Not a PDF or blocked): {pdf_url} [Content-Type: {content_type}]\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading PDF from {pdf_url}: {e}\")\n",
        "    return None\n",
        "\n",
        "# Function to rename the downloaded file based on title of the paper\n",
        "def sanitize_filename(title, version=\"\"):\n",
        "    return \"\".join(c if c.isalnum() or c in \"._-\" else \"_\" for c in title)[:100] + (f\"_{version}\" if version else \"\")\n",
        "\n",
        "\n",
        "# Function to download the papers after checking if it is a direct link/doi/webpage\n",
        "def download_all_pdfs(results):\n",
        "    os.makedirs(\"downloads\", exist_ok=True)\n",
        "    #all_papers = results[\"EUROPEPMC\"] + results[\"DOAJ\"] + results[\"SEMANTIC\"]\n",
        "    all_papers = []\n",
        "    for source in [\"EUROPEPMC\", \"DOAJ\", \"SEMANTIC\"]:\n",
        "        all_papers.extend(results[source])\n",
        "\n",
        "    for idx, paper in enumerate(all_papers):\n",
        "        title = paper.get(\"title\", f\"paper_{idx}\")\n",
        "        pdf_url = paper.get(\"full_text_url\")\n",
        "\n",
        "        if pdf_url:\n",
        "            filename = sanitize_filename(title) + \".pdf\"\n",
        "            save_path = os.path.join(\"downloads\", filename)\n",
        "            print(f\"\\nDownloading Paper {idx + 1}: {title}\")\n",
        "            resolve_pdf_url_and_download(pdf_url, save_path)\n",
        "\n",
        "\n",
        "# Function to resolve the pdf url correctly based on api response and download the paper accordingly\n",
        "def resolve_pdf_url_and_download(full_text_url, save_path):\n",
        "\n",
        "    # Direct PDF check\n",
        "    if is_direct_pdf_link(full_text_url):\n",
        "        return download_pdf_from_url(full_text_url, save_path)\n",
        "\n",
        "    # Check if link is DOI and use Unpaywall to download\n",
        "    parsed = urlparse(full_text_url)\n",
        "    if \"doi.org\" in parsed.netloc:\n",
        "        doi = parsed.path.strip(\"/\")\n",
        "        print(f\"Using Unpaywall for DOI: {doi}\")\n",
        "        pdf_url = get_pdf_from_unpaywall(doi)\n",
        "        if pdf_url and is_direct_pdf_link(pdf_url):\n",
        "            return download_pdf_from_url(pdf_url, save_path)\n",
        "\n",
        "    # Scrape the webpage to find PDF url and download\n",
        "    print(f\"Scraping HTML for PDF: {full_text_url}\")\n",
        "    pdf_url = extract_pdf_link_from_html_page(full_text_url)\n",
        "    if pdf_url and is_direct_pdf_link(pdf_url):\n",
        "        return download_pdf_from_url(pdf_url, save_path)\n",
        "\n",
        "    print(f\"No downloadable PDF found\")\n",
        "    return\n",
        "\n",
        "print(\"\\n Welcome to the PRISMA-ScR Automated Research Tool!\")\n",
        "query = input(\"Enter keywords to search for research papers: \")\n",
        "\n",
        "print(\"Searching PubMed, DOAJ, SEMANTIC SCHOLAR databases... Please wait...\\n\")\n",
        "search_results = parallel_search(query, max_results=30)\n",
        "\n",
        "download_all_pdfs(search_results)\n",
        "print(\"Search complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Research question -mapping where and how mobile apps have been used as part of natural disaster mental health response strategies.\n",
        "\n"
      ],
      "metadata": {
        "id": "RZ6ImQx9pk16"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Ensemble Model"
      ],
      "metadata": {
        "id": "qIp9R0ZBbmYj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ensemble model\n",
        "Check the model apis, use the genfuser and pairrank paper to get ideas and come up with prompts"
      ],
      "metadata": {
        "id": "1CcXkQDCXbV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert-score mistralai"
      ],
      "metadata": {
        "id": "__B3W8O2dbr5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e5c279a-40f3-44b2-fb53-41f48ff68dcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bert-score\n",
            "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting mistralai\n",
            "  Downloading mistralai-1.7.0-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.6.0+cu124)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.2.2)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.51.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert-score) (3.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert-score) (24.2)\n",
            "Collecting eval-type-backport>=0.2.0 (from mistralai)\n",
            "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: httpx>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from mistralai) (0.28.1)\n",
            "Requirement already satisfied: pydantic>=2.10.3 in /usr/local/lib/python3.11/dist-packages (from mistralai) (2.11.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from mistralai) (2.9.0.post0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mistralai) (0.4.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->mistralai) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->mistralai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->mistralai) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->mistralai) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.28.1->mistralai) (0.16.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10.3->mistralai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10.3->mistralai) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10.3->mistralai) (4.13.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->mistralai) (1.17.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.0.0->bert-score)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.0.0->bert-score)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.0.0->bert-score)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.0.0->bert-score)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.0.0->bert-score)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.0.0->bert-score)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.0.0->bert-score)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.0.0->bert-score)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.0.0->bert-score)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.0.0->bert-score)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert-score) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.30.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.5.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (3.2.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2.4.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.28.1->mistralai) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.2)\n",
            "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mistralai-1.7.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.5/301.5 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, eval-type-backport, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, mistralai, bert-score\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed bert-score-0.3.13 eval-type-backport-0.2.2 mistralai-1.7.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------FINAL CODE ENSEMBLE-------------------#\n",
        "import numpy as np\n",
        "import json\n",
        "import requests\n",
        "import torch\n",
        "from google import genai\n",
        "from bert_score import score as bert_score\n",
        "from mistralai import Mistral\n",
        "\n",
        "def query_gemini(prompt, api_key):\n",
        "    client = genai.Client(api_key=api_key)\n",
        "    try:\n",
        "        response = client.models.generate_content(\n",
        "          model=\"gemini-2.0-flash\", contents=prompt\n",
        "        )\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        print(f\"Error querying Gemini API: {e}\")\n",
        "        return None\n",
        "\n",
        "def query_mistral(prompt, api_token):\n",
        "    try:\n",
        "        model= \"mistral-small-latest\"\n",
        "        print(\"Sending prompt to Mistral API...\")\n",
        "        client = Mistral(api_key=api_token)\n",
        "        chat_response = client.chat.complete(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": prompt,\n",
        "                }\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        response_content = chat_response.choices[0].message.content\n",
        "\n",
        "        # Try to parse as JSON\n",
        "        try:\n",
        "            json_response = json.loads(response_content)\n",
        "            return json_response\n",
        "        except json.JSONDecodeError:\n",
        "            print(\"Warning: Response is not valid JSON. Returning raw text.\")\n",
        "            return response_content\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error querying Mistral API: {e}\")\n",
        "        return None\n",
        "def parse_ranking_response(response_text):\n",
        "    try:\n",
        "        # Try to extract JSON from the response\n",
        "        start_idx = response_text.find('{')\n",
        "        end_idx = response_text.rfind('}')\n",
        "\n",
        "        if start_idx >= 0 and end_idx > start_idx:\n",
        "            json_str = response_text[start_idx:end_idx+1]\n",
        "            ranking_data = json.loads(json_str)\n",
        "\n",
        "            if isinstance(ranking_data, dict) and \"ranking\" in ranking_data:\n",
        "                return ranking_data[\"ranking\"], ranking_data.get(\"scores\", [])\n",
        "\n",
        "        # Fallback to parsing numbered list\n",
        "        lines = response_text.strip().split('\\n')\n",
        "        ranking = []\n",
        "\n",
        "        for line in lines:\n",
        "            if line and ':' in line:\n",
        "                parts = line.split(':', 1)\n",
        "                try:\n",
        "                    index = int(parts[0].strip().rstrip('.')) - 1\n",
        "                    ranking.append(index)\n",
        "                except ValueError:\n",
        "                    pass\n",
        "\n",
        "        return ranking, [] if ranking else None, []\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing ranking response: {e}\")\n",
        "        return None, []\n",
        "def get_model_rankings(abstracts, titles, query, gemini_api_key, hf_api_token):\n",
        "\n",
        "\n",
        "    abstract_list = \"\"\n",
        "    for i, abs in enumerate(abstracts):\n",
        "        abstract_list += f\"{i+1}. {abs}\\n\"  # If abs is a string. Adjust if dict.\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "      You are a research assistant helping with a scoping review.\n",
        "\n",
        "      RESEARCH QUESTION: {query}\n",
        "\n",
        "      I have retrieved the following {len(abstracts)} papers. Please rank them based on their relevance to the research question:\n",
        "\n",
        "      {abstract_list}\n",
        "\n",
        "      Based on their abstracts, rank these papers in order of relevance to my research question.\n",
        "      For each paper, assign a relevance score from 0 to 10, where 10 is most relevant.\n",
        "\n",
        "      Return your answer as a JSON object with the following format:\n",
        "      {{\n",
        "        \"ranking\": [list of paper numbers in order of relevance from most to least relevant],\n",
        "        \"scores\": [corresponding relevance scores for each paper]\n",
        "      }}\n",
        "\n",
        "      Provide ONLY the JSON response with no additional text.\n",
        "      \"\"\"\n",
        "    gemini_response = query_gemini(prompt, gemini_api_key)\n",
        "    mistral_response = query_mistral(prompt, hf_api_token)\n",
        "    gemini_ranking, gemini_scores = parse_ranking_response(gemini_response) if gemini_response else (None, [])\n",
        "    mistral_ranking, mistral_scores = parse_ranking_response(mistral_response) if mistral_response else (None, [])\n",
        "\n",
        "    results = {\n",
        "        \"gemini\": {\n",
        "            \"ranking\": gemini_ranking if gemini_ranking else [],\n",
        "            \"scores\": gemini_scores if gemini_scores else []\n",
        "        },\n",
        "        \"mistral\": {\n",
        "            \"ranking\": mistral_ranking if mistral_ranking else [],\n",
        "            \"scores\": mistral_scores if mistral_scores else []\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return results\n",
        "\n",
        "def get_ensemble_ranking(abstracts, titles, query, results_from_models=None):\n",
        "\n",
        "    if not abstracts:\n",
        "        return np.arange(len(abstracts)), np.zeros(len(abstracts))\n",
        "\n",
        "    if results_from_models is None:\n",
        "        gemini_api_key = \"AIzaSyDApKHVHxDERiaZDit0Dfpz9XMdQdhL36c\"\n",
        "        hf_api_token = \"Bzj8ue6SoITkGj4hUcRxb9sp56k4aiUa\"\n",
        "        results_from_models = get_model_rankings(abstracts, titles, query, gemini_api_key, hf_api_token)\n",
        "\n",
        "    gemini_ranking = np.array(results_from_models[\"gemini\"][\"ranking\"]) if results_from_models[\"gemini\"][\"ranking\"] else np.arange(len(abstracts))\n",
        "    mistral_ranking = np.array(results_from_models[\"mistral\"][\"ranking\"]) if results_from_models[\"mistral\"][\"ranking\"] else np.arange(len(abstracts))\n",
        "\n",
        "    if len(gemini_ranking) != len(abstracts):\n",
        "        gemini_ranking = np.arange(len(abstracts))\n",
        "    if len(mistral_ranking) != len(abstracts):\n",
        "        mistral_ranking = np.arange(len(abstracts))\n",
        "\n",
        "    ensemble_scores = np.zeros(len(abstracts))\n",
        "    for i in range(len(abstracts)):\n",
        "        gemini_position = np.where(gemini_ranking == i)[0][0] if i in gemini_ranking else len(abstracts)\n",
        "        mistral_position = np.where(mistral_ranking == i)[0][0] if i in mistral_ranking else len(abstracts)\n",
        "\n",
        "        ensemble_scores[i] = 1 / (1 + (gemini_position + mistral_position) / 2)\n",
        "\n",
        "    ensemble_ranked = np.argsort(ensemble_scores)[::-1]\n",
        "\n",
        "    return ensemble_ranked, ensemble_scores\n",
        "\n",
        "def get_model_summaries(abstracts, titles, ranking, query, gemini_api_key, hf_api_token, top_n=5):\n",
        "\n",
        "    top_indices = ranking[:min(top_n, len(ranking))]\n",
        "\n",
        "    papers_info = []\n",
        "    for i, idx in enumerate(top_indices):\n",
        "        paper_info = f\"{i+1}: \\\"{titles[idx]}\\\"\\n\"\n",
        "        paper_info += f\"Abstract: {abstracts[idx]}\\n\"\n",
        "        papers_info.append(paper_info)\n",
        "\n",
        "    papers_text = \"\\n\\n\".join(papers_info)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are a research assistant helping with a scoping review, following PRISMA Guidelines.\n",
        "    RESEARCH QUESTION: {query}\n",
        "    Based on the top {len(top_indices)} papers below, create a comprehensive summary that:\n",
        "    1. Identifies key themes and findings across the papers\n",
        "    2. Highlights methodological approaches used\n",
        "    3. Notes any gaps in the literature\n",
        "    4. Suggests directions for future research\n",
        "    Papers:\n",
        "    {papers_text}\n",
        "    Provide ONLY a well-structured summary that synthesizes the information from these papers with no additional text.\n",
        "    \"\"\"\n",
        "\n",
        "    gemini_summary = query_gemini(prompt, gemini_api_key)\n",
        "    mistral_summary = query_mistral(prompt, hf_api_token)\n",
        "\n",
        "    return {\n",
        "        \"gemini\": gemini_summary,\n",
        "        \"mistral\": mistral_summary\n",
        "    }\n",
        "\n",
        "def get_ensemble_summaries(abstracts, titles, query, results_from_models=None):\n",
        "\n",
        "    if not abstracts:\n",
        "        return \"No abstracts provided for summarization.\"\n",
        "\n",
        "    ensemble_ranked, _ = get_ensemble_ranking(abstracts, titles, query, results_from_models)\n",
        "\n",
        "    gemini_api_key = \"AIzaSyDApKHVHxDERiaZDit0Dfpz9XMdQdhL36c\"\n",
        "    hf_api_token = \"Bzj8ue6SoITkGj4hUcRxb9sp56k4aiUa\"\n",
        "\n",
        "    summaries = get_model_summaries(abstracts, titles, ensemble_ranked, query, gemini_api_key, hf_api_token)\n",
        "\n",
        "    if summaries[\"gemini\"] and summaries[\"mistral\"]:\n",
        "        ensemble_prompt = f\"\"\"\n",
        "        You are a research assistant helping with a scoping review, following PRISMA guidelines.\n",
        "\n",
        "        I have two summaries of the same set of papers related to this research question: \"{query}\"\n",
        "\n",
        "        Summary 1:\n",
        "        {summaries[\"gemini\"]}\n",
        "\n",
        "        Summary 2:\n",
        "        {summaries[\"mistral\"]}\n",
        "\n",
        "        Please create a synthesis of these two summaries, incorporating the strongest insights and analysis from each.\n",
        "        The final summary should be comprehensive yet concise, highlighting key themes, methods, gaps, and future directions.\n",
        "        Do not reference these summaries and just output the final summary.\n",
        "        \"\"\"\n",
        "\n",
        "        ensemble_summary = query_gemini(ensemble_prompt, gemini_api_key)\n",
        "        return ensemble_summary\n",
        "    elif summaries[\"gemini\"]:\n",
        "        return summaries[\"gemini\"]\n",
        "    elif summaries[\"mistral\"]:\n",
        "        return summaries[\"mistral\"]\n",
        "    else:\n",
        "        return \"Unable to generate summaries from the provided models.\"\n",
        "\n",
        "def get_model_summaries_for_each_paper(abstracts, titles, query, gemini_api_key, hf_api_token):\n",
        "\n",
        "    gemini_summary = \"\"\n",
        "    mistral_summary = \"\"\n",
        "    for i in range(len(abstracts)):\n",
        "      prompt = f\"\"\"\n",
        "      Summarize this research paper: Title - {titles[i]}, abstract - {abstracts[i]}\n",
        "      Provide ONLY a well-structured summary that synthesizes the information from these papers with no additional text.\n",
        "      \"\"\"\n",
        "\n",
        "      gemini_summary += (query_gemini(prompt, gemini_api_key))\n",
        "      mistral_summary +=  (query_mistral(prompt, hf_api_token))\n",
        "      time.sleep(1)\n",
        "    return {\n",
        "        \"gemini_ep\": gemini_summary,\n",
        "        \"mistral_ep\": mistral_summary\n",
        "    }\n",
        "\n",
        "def get_ensemble_summaries_for_each_paper(abstracts, titles, query, results_from_models=None):\n",
        "\n",
        "    if not abstracts:\n",
        "        return \"No abstracts provided for summarization.\"\n",
        "    gemini_api_key = \"AIzaSyDApKHVHxDERiaZDit0Dfpz9XMdQdhL36c\"\n",
        "    hf_api_token = \"Bzj8ue6SoITkGj4hUcRxb9sp56k4aiUa\"\n",
        "\n",
        "    summaries = get_model_summaries_for_each_paper(abstracts, titles, query, gemini_api_key, hf_api_token)\n",
        "\n",
        "    if summaries[\"gemini_ep\"] and summaries[\"mistral_ep\"]:\n",
        "        ensemble_prompt = f\"\"\"\n",
        "        I have two summaries of the same set of papers related to this research question: \"{query}\"\n",
        "\n",
        "        Summary 1:\n",
        "        {summaries[\"gemini_ep\"]}\n",
        "\n",
        "        Summary 2:\n",
        "        {summaries[\"mistral_ep\"]}\n",
        "\n",
        "        Please create a synthesis of these two summaries maintaining academic standard. Do not reference these summaries and just output the final summary.\n",
        "        \"\"\"\n",
        "\n",
        "        ensemble_summary = query_gemini(ensemble_prompt, gemini_api_key)\n",
        "        return ensemble_summary\n",
        "    elif summaries[\"gemini_ep\"]:\n",
        "        return summaries[\"gemini_ep\"]\n",
        "    elif summaries[\"mistral_ep\"]:\n",
        "        return summaries[\"mistral_ep\"]\n",
        "    else:\n",
        "        return \"Unable to generate summaries from the provided models.\"\n",
        "\n",
        "def evaluate_summaries_with_bert(summaries, reference_summary):\n",
        "\n",
        "    scores = {}\n",
        "\n",
        "    for model_name, summary in summaries.items():\n",
        "        if summary:\n",
        "            try:\n",
        "                P, R, F1 = bert_score([summary], [reference_summary], lang=\"en\", rescale_with_baseline=True)\n",
        "                scores[model_name] = {\n",
        "                    \"precision\": P.item(),\n",
        "                    \"recall\": R.item(),\n",
        "                    \"f1\": F1.item()\n",
        "                }\n",
        "            except Exception as e:\n",
        "                print(f\"Error computing BERT Score for {model_name}: {e}\")\n",
        "                scores[model_name] = None\n",
        "\n",
        "    # If we have an ensemble summary, evaluate it too\n",
        "    if \"ensemble\" in summaries and summaries[\"ensemble\"]:\n",
        "        try:\n",
        "            P, R, F1 = bert_score([summaries[\"ensemble\"]], [reference_summary], lang=\"en\", rescale_with_baseline=True)\n",
        "            scores[\"ensemble\"] = {\n",
        "                \"precision\": P.item(),\n",
        "                \"recall\": R.item(),\n",
        "                \"f1\": F1.item()\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Error computing BERT Score for ensemble: {e}\")\n",
        "            scores[\"ensemble\"] = None\n",
        "\n",
        "    return scores"
      ],
      "metadata": {
        "id": "uJfGxxvodTpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Abstract Screening\n",
        "\n",
        "The abstracts of the relevant papers go through screening process below. Screening strategy is used by ranking the papers with sentence transformers and ensemble model. All the ranks obtained by each paper will have a final consolidated ranking from the models to minimise the model bias using Reciprocal Rank Fusion\n",
        "\n",
        "### Sentence Transformers used\n",
        "The following pretrained sentence transformers are used:\n",
        "* BM25\n",
        "* SBERT\n",
        "* SPLADE\n",
        "\n",
        "and Ensemble model of Deepseek and Gemini LLMs.\n",
        "\n",
        "The abstracts are ranked on the relevance similarity scores based on mean consolidated embeddings with other abstracts and research question. Say, we have 'N' total papers retrieved and we use the above models(m ∈ [1,4]),\\\n",
        "<center>$R_{abs_i}^{model_m} = SS_{model_m}(abs_i, \\frac{1}{2N}(Σ_{j!=i}abs_j)+rq/2)$</center>, where,\\\n",
        "\n",
        "* R_{abs_i}^{model_m} is the rank of paper i with respect to model m,\n",
        "* SS_{model_m}(a, b) is similarity score with respect to model m between a and b, a and b are two text embedding vectors,\n",
        "* abs_i is abstract embedding vector of paper i,\n",
        "* rq is embedding vector of research question.\n",
        "\n",
        "Now, we have m ranks for each paper, which can possibly include model bias because they are trained over different kinds of data.\n",
        "\n",
        "To reduce this bias, we adopt RRF:\n",
        "<center> $Rank_{abs_i} = Σ_{j=1}^m \\frac{1}{k+R_{abs_i}^{model_j} }$</center>\n",
        "where,\n",
        "\n",
        "\n",
        "* Rank_{abs_i} is the final rank of i^{th} paper,\n",
        "* k is a constant, generally used 60\n",
        "* R_{abs_i}^{model_j} is the rank of i^{th} paper with respect to model j.\n",
        "\n"
      ],
      "metadata": {
        "id": "oe7pUW6WQqOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znQEk7G936w0",
        "outputId": "24ca519e-24dc-40e3-cd70-9743c8e8a701"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.11/dist-packages (1.25.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXTENSIVE ABSTRACT RETRIEVAL"
      ],
      "metadata": {
        "id": "ZlLTbK8g4TmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from pathlib import Path\n",
        "import fitz  # PyMuPDF\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Download NLTK data if needed\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "def get_abstracts_from_papers(results):\n",
        "    \"\"\"\n",
        "    Extract abstracts from the search results.\n",
        "    If abstracts are missing, attempt to extract them from PDFs.\n",
        "\n",
        "    Args:\n",
        "        results: Dictionary with search results from different sources\n",
        "\n",
        "    Returns:\n",
        "        tuple: (abstracts, titles) lists containing all valid papers\n",
        "    \"\"\"\n",
        "    all_papers = results[\"EUROPEPMC\"] + results[\"DOAJ\"] + results[\"SEMANTIC\"]\n",
        "    abstracts = []\n",
        "    titles = []\n",
        "    papers_without_abstracts = []\n",
        "\n",
        "    # First pass: collect available abstracts and identify papers needing extraction\n",
        "    for paper in all_papers:\n",
        "        title = paper.get(\"title\", \"\")\n",
        "        abstract = paper.get(\"abstract\", \"\")\n",
        "\n",
        "        if title and title.strip():  # We require a title\n",
        "            if abstract and abstract.strip():  # Paper already has an abstract\n",
        "                abstracts.append(abstract)\n",
        "                titles.append(title)\n",
        "            else:\n",
        "                # Save papers needing abstract extraction\n",
        "                papers_without_abstracts.append(paper)\n",
        "\n",
        "    # Second pass: extract abstracts from PDFs for papers that need them\n",
        "    if papers_without_abstracts:\n",
        "        pdf_abstracts, pdf_titles = extract_abstracts_from_pdfs('/content/downloads', papers_without_abstracts)\n",
        "        abstracts.extend(pdf_abstracts)\n",
        "        titles.extend(pdf_titles)\n",
        "\n",
        "    return abstracts, titles\n",
        "\n",
        "def extract_abstracts_from_pdfs(folder_path, papers_without_abstracts):\n",
        "    \"\"\"\n",
        "    Extract abstracts from PDF files for papers with missing abstracts.\n",
        "\n",
        "    Args:\n",
        "        folder_path: Path to folder containing PDF files\n",
        "        papers_without_abstracts: List of paper dictionaries with missing abstracts\n",
        "\n",
        "    Returns:\n",
        "        tuple: (extracted_abstracts, corresponding_titles)\n",
        "    \"\"\"\n",
        "    folder = Path(folder_path)\n",
        "    extracted_abstracts = []\n",
        "    corresponding_titles = []\n",
        "\n",
        "    # Create a mapping from normalized titles to papers\n",
        "    title_to_paper = {normalize_title(paper.get(\"title\", \"\")): paper\n",
        "                      for paper in papers_without_abstracts if paper.get(\"title\")}\n",
        "\n",
        "    # Process each PDF file in the folder\n",
        "    pdf_files = list(folder.glob('*.pdf'))\n",
        "    for pdf_path in pdf_files:\n",
        "        try:\n",
        "            # Extract title from PDF for matching\n",
        "            pdf_title = extract_title_from_pdf(pdf_path)\n",
        "\n",
        "            if pdf_title:\n",
        "                normalized_pdf_title = normalize_title(pdf_title)\n",
        "\n",
        "                # Try to match with papers that need abstracts\n",
        "                matched_paper = None\n",
        "\n",
        "                # Try exact title match first\n",
        "                if normalized_pdf_title in title_to_paper:\n",
        "                    matched_paper = title_to_paper[normalized_pdf_title]\n",
        "                else:\n",
        "                    # Try fuzzy matching\n",
        "                    best_match = find_best_title_match(normalized_pdf_title, title_to_paper.keys())\n",
        "                    if best_match:\n",
        "                        matched_paper = title_to_paper[best_match]\n",
        "\n",
        "                # If we found a match, extract the abstract\n",
        "                if matched_paper:\n",
        "                    abstract = extract_abstract_from_pdf(pdf_path)\n",
        "                    if abstract:\n",
        "                        extracted_abstracts.append(abstract)\n",
        "                        corresponding_titles.append(matched_paper.get(\"title\", \"\"))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {pdf_path.name}: {e}\")\n",
        "\n",
        "    return extracted_abstracts, corresponding_titles\n",
        "\n",
        "def normalize_title(title):\n",
        "    \"\"\"Normalize title for comparison by lowercasing and removing extra spaces.\"\"\"\n",
        "    return re.sub(r'\\s+', ' ', title.lower().strip())\n",
        "\n",
        "def find_best_title_match(pdf_title, candidate_titles, threshold=0.7):\n",
        "    \"\"\"Find the best matching title using token-based similarity.\"\"\"\n",
        "    pdf_title_words = set(re.findall(r'\\b\\w+\\b', pdf_title.lower()))\n",
        "\n",
        "    best_match = None\n",
        "    best_score = 0\n",
        "\n",
        "    for candidate in candidate_titles:\n",
        "        candidate_words = set(re.findall(r'\\b\\w+\\b', candidate.lower()))\n",
        "        if pdf_title_words and candidate_words:\n",
        "            # Calculate Jaccard similarity\n",
        "            intersection = len(pdf_title_words.intersection(candidate_words))\n",
        "            union = len(pdf_title_words.union(candidate_words))\n",
        "            score = intersection / union if union > 0 else 0\n",
        "\n",
        "            if score > threshold and score > best_score:\n",
        "                best_score = score\n",
        "                best_match = candidate\n",
        "\n",
        "    return best_match\n",
        "\n",
        "def extract_title_from_pdf(pdf_path):\n",
        "    \"\"\"Extract title from a PDF file.\"\"\"\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "\n",
        "        # Try to get title from metadata\n",
        "        metadata = doc.metadata\n",
        "        if metadata.get(\"title\") and len(metadata.get(\"title\").strip()) > 5:\n",
        "            doc.close()\n",
        "            return metadata.get(\"title\").strip()\n",
        "\n",
        "        # Extract from first page\n",
        "        text = doc[0].get_text()\n",
        "        doc.close()\n",
        "\n",
        "        # Title is usually the first substantial line\n",
        "        lines = [line.strip() for line in text.split('\\n') if len(line.strip()) > 5]\n",
        "        for line in lines:\n",
        "            if 10 <= len(line) <= 200 and not line.lower().startswith(('doi', 'http', 'www')):\n",
        "                return line\n",
        "\n",
        "        return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting title from PDF: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_abstract_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Extract abstract from a PDF file.\n",
        "\n",
        "    Args:\n",
        "        pdf_path: Path to the PDF file\n",
        "\n",
        "    Returns:\n",
        "        str or None: Extracted abstract or None if not found\n",
        "    \"\"\"\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "        text = \"\"\n",
        "\n",
        "        # Get text from first few pages where abstract is likely to be\n",
        "        for page_num in range(min(3, len(doc))):\n",
        "            text += doc[page_num].get_text()\n",
        "        doc.close()\n",
        "\n",
        "        return find_abstract_in_text(text)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting abstract from PDF: {e}\")\n",
        "        return None\n",
        "\n",
        "def find_abstract_in_text(text):\n",
        "    \"\"\"\n",
        "    Find abstract in the PDF text using multiple strategies.\n",
        "\n",
        "    Args:\n",
        "        text: Extracted text from PDF\n",
        "\n",
        "    Returns:\n",
        "        str or None: Extracted abstract or None if not found\n",
        "    \"\"\"\n",
        "    # Method 1: Look for specifically labeled abstract section\n",
        "    abstract_patterns = [\n",
        "        r\"(?i)abstract[\\s]*[:.\\n]+(.*?)(?:[\\n]{2,}|\\b(?:introduction|keywords|key\\s+words)\\b)\",\n",
        "        r\"(?i)ABSTRACT[\\s]*[:.\\n]+(.*?)(?:[\\n]{2,}|\\b(?:introduction|keywords|key\\s+words)\\b)\",\n",
        "        r\"(?i)Abstract[\\s]*[:.\\n]+(.*?)(?:[\\n]{2,}|\\b(?:introduction|keywords|key\\s+words)\\b)\",\n",
        "        r\"(?i)Summary[\\s]*[:.\\n]+(.*?)(?:[\\n]{2,}|\\b(?:introduction|keywords|key\\s+words)\\b)\",\n",
        "        r\"(?i)SUMMARY[\\s]*[:.\\n]+(.*?)(?:[\\n]{2,}|\\b(?:introduction|keywords|key\\s+words)\\b)\"\n",
        "    ]\n",
        "\n",
        "    for pattern in abstract_patterns:\n",
        "        match = re.search(pattern, text, re.DOTALL)\n",
        "        if match:\n",
        "            abstract = match.group(1).strip()\n",
        "            # Clean up the abstract (remove extra whitespaces, line breaks, etc.)\n",
        "            abstract = re.sub(r'\\s+', ' ', abstract)\n",
        "            return abstract\n",
        "\n",
        "    # Method 2: For academic papers, abstract is often the second paragraph after the title\n",
        "    paragraphs = re.split(r'\\n{2,}', text)\n",
        "    if len(paragraphs) > 2:\n",
        "        # If second paragraph is short (likely authors/affiliations), try third paragraph\n",
        "        candidate = paragraphs[1].strip()\n",
        "        if len(candidate.split()) < 30 and len(paragraphs) > 3:\n",
        "            candidate = paragraphs[0].strip()\n",
        "\n",
        "        # Clean up and verify it looks like an abstract\n",
        "        candidate = re.sub(r'\\s+', ' ', candidate)\n",
        "        words = candidate.split()\n",
        "        if 30 < len(words) < 500:  # Typical abstract length\n",
        "            return ' '.join(words)\n",
        "\n",
        "    # Method 3: Look for the first paragraph that looks like an abstract (sentence-based approach)\n",
        "    paragraphs = [re.sub(r'\\s+', ' ', p.strip()) for p in re.split(r'\\n{2,}', text) if p.strip()]\n",
        "    for paragraph in paragraphs[:5]:  # Check first 5 paragraphs\n",
        "        sentences = sent_tokenize(paragraph)\n",
        "        # Abstract usually has multiple sentences and meaningful length\n",
        "        if len(sentences) >= 2 and 50 < len(paragraph) < 2000:\n",
        "            return paragraph\n",
        "\n",
        "    return None"
      ],
      "metadata": {
        "id": "8po-6WpOl98C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rank_bm25"
      ],
      "metadata": {
        "id": "g36geKZ-0vVA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3f49b02-d5f4-461d-898a-6177b4197be3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rank_bm25 in /usr/local/lib/python3.11/dist-packages (0.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rank_bm25) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Reciprocal Rank Fusion (RRF) Function\n",
        "def reciprocal_rank_fusion(ranked_lists, k=60):\n",
        "    scores = {}\n",
        "    for rank_list in ranked_lists:\n",
        "        for rank, doc_id in enumerate(rank_list):\n",
        "            scores[doc_id] = scores.get(doc_id, 0) + 1 / (k + rank + 1)\n",
        "    return sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# BM25 Ranking\n",
        "def get_bm25_ranking(abstracts, query):\n",
        "    tokenized_abstracts = [doc.split() for doc in abstracts]\n",
        "    bm25 = BM25Okapi(tokenized_abstracts)\n",
        "    bm25_scores = bm25.get_scores(query.split())\n",
        "    return np.argsort(bm25_scores)[::-1], bm25_scores\n",
        "\n",
        "# SBERT Ranking\n",
        "def get_sbert_ranking(abstracts, query):\n",
        "    sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    abstract_embeddings = sbert_model.encode(abstracts, convert_to_tensor=True)\n",
        "    query_embedding = sbert_model.encode([query], convert_to_tensor=True)\n",
        "\n",
        "    # Calculate mean abstract embedding to use in similarity calculation\n",
        "    mean_abstract_embedding = torch.mean(abstract_embeddings, dim=0, keepdim=True)\n",
        "    # Combine mean abstract and research question as described in the formula\n",
        "    combined_embedding = 0.5 * mean_abstract_embedding + 0.5 * query_embedding\n",
        "\n",
        "    # Calculate similarity scores for each abstract with the combined embedding\n",
        "    abstract_embeddings = abstract_embeddings.cpu().numpy()\n",
        "    combined_embedding = combined_embedding.cpu().numpy()\n",
        "    sbert_scores = cosine_similarity(abstract_embeddings, combined_embedding).flatten()\n",
        "\n",
        "    return np.argsort(sbert_scores)[::-1], sbert_scores\n",
        "\n",
        "# SPLADE Ranking\n",
        "def get_splade_ranking(abstracts, query):\n",
        "    try:\n",
        "        splade_tokenizer = AutoTokenizer.from_pretrained(\"naver/splade-cocondenser-ensembledistil\")\n",
        "        splade_model = AutoModel.from_pretrained(\"naver/splade-cocondenser-ensembledistil\")\n",
        "\n",
        "        def get_splade_representation(text):\n",
        "            inputs = splade_tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "            with torch.no_grad():\n",
        "                outputs = splade_model(**inputs).last_hidden_state.mean(dim=1)\n",
        "            return outputs.squeeze().cpu().numpy()\n",
        "\n",
        "        splade_embeddings = np.array([get_splade_representation(text) for text in abstracts])\n",
        "        query_splade_embedding = get_splade_representation(query)\n",
        "\n",
        "        # Calculate mean abstract embedding\n",
        "        mean_splade_embedding = np.mean(splade_embeddings, axis=0)\n",
        "        # Combine mean abstract and research question\n",
        "        combined_embedding = 0.5 * mean_splade_embedding + 0.5 * query_splade_embedding\n",
        "\n",
        "        splade_scores = cosine_similarity(splade_embeddings, combined_embedding.reshape(1, -1)).flatten()\n",
        "        return np.argsort(splade_scores)[::-1], splade_scores\n",
        "    except Exception as e:\n",
        "        print(f\"Error in SPLADE ranking: {e}\")\n",
        "        # Return dummy ranking if SPLADE fails\n",
        "        return np.arange(len(abstracts)), np.zeros(len(abstracts))\n",
        "\n",
        "def printRankings(ranked_results):\n",
        "  for i in ranked_results:\n",
        "    print(f'Rank: {i[\"rank\"]}       Title:{i[\"title\"]}       Relevance Score:{i[\"relevance_score\"]}')\n",
        "    print(\"\\n\")\n",
        "\n",
        "def rank_abstracts(search_results, research_question, rfm=None):\n",
        "    # Extract abstracts from search results\n",
        "    abstracts, titles = get_abstracts_from_papers(search_results)\n",
        "\n",
        "    if not abstracts:\n",
        "        print(\"No abstracts found in the search results\")\n",
        "        return []\n",
        "\n",
        "    print(f\"Ranking {len(abstracts)} abstracts based on relevance to research question...\")\n",
        "\n",
        "    # Get rankings from each model\n",
        "    bm25_ranked, bm25_scores = get_bm25_ranking(abstracts, research_question)\n",
        "    sbert_ranked, sbert_scores = get_sbert_ranking(abstracts, research_question)\n",
        "    splade_ranked, splade_scores = get_splade_ranking(abstracts, research_question)\n",
        "\n",
        "    # Collect results from the first three models to use in ensemble\n",
        "    results_from_models = {\n",
        "        \"bm25\": {\"ranking\": bm25_ranked.tolist(), \"scores\": bm25_scores.tolist()},\n",
        "        \"sbert\": {\"ranking\": sbert_ranked.tolist(), \"scores\": sbert_scores.tolist()},\n",
        "        \"splade\": {\"ranking\": splade_ranked.tolist(), \"scores\": splade_scores.tolist()}\n",
        "    }\n",
        "\n",
        "    # Get ensemble model ranking\n",
        "    ensemble_ranked, ensemble_scores = get_ensemble_ranking(abstracts, titles, research_question, rfm)\n",
        "\n",
        "    # Apply RRF to Combine Rankings\n",
        "    ranked_lists = [bm25_ranked, sbert_ranked, splade_ranked, ensemble_ranked]\n",
        "    final_ranking = reciprocal_rank_fusion(ranked_lists)\n",
        "\n",
        "    # Create the final ranked results\n",
        "    ranked_results = []\n",
        "    for idx, (doc_id, score) in enumerate(final_ranking):\n",
        "        if doc_id < len(titles):  # Ensure valid index\n",
        "            ranked_results.append({\n",
        "                \"rank\": idx + 1,\n",
        "                \"title\": titles[doc_id],\n",
        "                \"abstract\": abstracts[doc_id],\n",
        "                \"relevance_score\": score\n",
        "            })\n",
        "\n",
        "    # Save rankings for later use\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "    with open(\"results/abstract_rankings.json\", \"w\") as f:\n",
        "        json.dump(ranked_results, f, indent=2)\n",
        "\n",
        "    return ranked_results"
      ],
      "metadata": {
        "id": "kiO3DObMWgz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ranking the abstracts"
      ],
      "metadata": {
        "id": "W48dAFc8DzCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------CODE TO RUN THE PIPELINE----------------------#\n",
        "def run_review_pipeline(query, abstracts, titles, reference_summary=None):\n",
        "\n",
        "    gemini_api_key = \"AIzaSyDApKHVHxDERiaZDit0Dfpz9XMdQdhL36c\"\n",
        "    hf_api_token = \"Bzj8ue6SoITkGj4hUcRxb9sp56k4aiUa\"\n",
        "\n",
        "    print(\"Getting model rankings...\")\n",
        "    model_rankings = get_model_rankings(abstracts, titles, query, gemini_api_key, hf_api_token)\n",
        "\n",
        "    print(\"Creating ensemble ranking...\")\n",
        "    ensemble_ranked, ensemble_scores = get_ensemble_ranking(abstracts, titles, query, model_rankings)\n",
        "\n",
        "    print(\"Final ranking...\")\n",
        "    final_ranking = rank_abstracts(search_results, query, model_rankings)\n",
        "\n",
        "    print(\"Generating model summaries...\")\n",
        "    model_summaries = get_model_summaries(abstracts, titles, ensemble_ranked, query, gemini_api_key, hf_api_token)\n",
        "\n",
        "    print(\"Creating ensemble summary...\")\n",
        "    ensemble_summary = get_ensemble_summaries(abstracts, titles, query, model_rankings)\n",
        "\n",
        "    print(\"Generating model summaries for each paper...\")\n",
        "    model_summaries_ep = get_model_summaries_for_each_paper(abstracts, titles, query, gemini_api_key, hf_api_token)\n",
        "\n",
        "    print(\"Creating ensemble summary for each paper..\")\n",
        "    ensemble_summary_ep = get_ensemble_summaries_for_each_paper(abstracts, titles, query, model_rankings)\n",
        "\n",
        "    all_summaries = {\n",
        "        \"gemini\": model_summaries[\"gemini\"],\n",
        "        \"mistral\": model_summaries[\"mistral\"],\n",
        "        \"ensemble\": ensemble_summary,\n",
        "        \"gemini_ep\": model_summaries_ep[\"gemini_ep\"],\n",
        "        \"mistral_ep\": model_summaries_ep[\"mistral_ep\"],\n",
        "        \"ensemble_ep\": ensemble_summary_ep\n",
        "    }\n",
        "\n",
        "    results = {\n",
        "        \"rankings\": {\n",
        "            \"gemini\": model_rankings[\"gemini\"],\n",
        "            \"mistral\": model_rankings[\"mistral\"],\n",
        "            \"ensemble\": {\n",
        "                \"ranking\": ensemble_ranked.tolist(),\n",
        "                \"scores\": ensemble_scores.tolist()\n",
        "            },\n",
        "            \"final\" : final_ranking\n",
        "        },\n",
        "        \"summaries\": all_summaries\n",
        "    }\n",
        "\n",
        "    # Step 5: Evaluate with BERT Score if reference is provided\n",
        "    if reference_summary:\n",
        "        print(\"Evaluating summaries with BERT Score...\")\n",
        "        bert_scores = evaluate_summaries_with_bert(all_summaries, reference_summary)\n",
        "        results[\"evaluation\"] = bert_scores\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "ayTLrQUYlPGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ref_summary = \"\"\"\n",
        "Background\n",
        "Disasters are becoming more frequent due to the impact of extreme weather events attributed to climate change, causing loss of lives, property, and psychological trauma. Mental health response to disasters emphasizes prevention and mitigation, and mobile health (mHealth) apps have been used for mental health promotion and treatment. However, little is known about their use in the mental health components of disaster management.\n",
        "\n",
        "Objective\n",
        "This scoping review was conducted to explore the use of mobile phone apps for mental health responses to natural disasters and to identify gaps in the literature.\n",
        "\n",
        "Methods\n",
        "We identified relevant keywords and subject headings and conducted comprehensive searches in 6 electronic databases. Studies in which participants were exposed to a man-made disaster were included if the sample also included some participants exposed to a natural hazard. Only full-text studies published in English were included. The initial titles and abstracts of the unique papers were screened by 2 independent review authors. Full texts of the selected papers that met the inclusion criteria were reviewed by the 2 independent reviewers. Data were extracted from each selected full-text paper and synthesized using a narrative approach based on the outcome measures, duration, frequency of use of the mobile phone apps, and the outcomes. This scoping review was reported according to the PRISMA-ScR (Preferred Reporting Items for Systematic Reviews and Meta-Analyses extension for Scoping Reviews).\n",
        "\n",
        "Results\n",
        "Of the 1398 papers retrieved, 5 were included in this review. A total of 3 studies were conducted on participants exposed to psychological stress following a disaster while 2 were for disaster relief workers. The mobile phone apps for the interventions included Training for Life Skills, Sonoma Rises, Headspace, Psychological First Aid, and Substance Abuse and Mental Health Services Administration (SAMHSA) Behavioural Health Disaster Response Apps. The different studies assessed the effectiveness or efficacy of the mobile app, feasibility, acceptability, and characteristics of app use or predictors of use. Different measures were used to assess the effectiveness of the apps’ use as either the primary or secondary outcome.\n",
        "\n",
        "Conclusions\n",
        "A limited number of studies are exploring the use of mobile phone apps for mental health responses to disasters. The 5 studies included in this review showed promising results. Mobile apps have the potential to provide effective mental health support before, during, and after disasters. However, further research is needed to explore the potential of mobile phone apps in mental health responses to all hazards.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "eX8gE_rdeXE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abstracts, titles = get_abstracts_from_papers(search_results)\n",
        "results = run_review_pipeline(query, abstracts, titles, ref_summary)\n",
        "\n",
        "# Print rankings\n",
        "print(\"\\nPaper Rankings:\")\n",
        "print(\"==============\")\n",
        "print(\"\\nGemini Ranking:\")\n",
        "# print(\"results!!!----------->\", results)\n",
        "for i, idx in enumerate(results[\"rankings\"][\"gemini\"][\"ranking\"]):\n",
        "    print(f\"{i+1}. {titles[idx-1]}\")\n",
        "\n",
        "print(\"\\nMistral Ranking:\")\n",
        "for i, idx in enumerate(results[\"rankings\"][\"mistral\"][\"ranking\"]):\n",
        "    print(f\"{i+1}. {titles[idx-1]}\")\n",
        "\n",
        "print(\"\\nEnsemble Ranking:\")\n",
        "for i, idx in enumerate(results[\"rankings\"][\"ensemble\"][\"ranking\"]):\n",
        "    print(f\"{i+1}. {titles[idx]} (Score: {results['rankings']['ensemble']['scores'][i]:.4f})\")\n",
        "\n",
        "print(\"\\nFinal Ranking:\")\n",
        "for i in (results[\"rankings\"][\"final\"]):\n",
        "  print(f\"{i['rank']}.{i['title']} (Score: {i['relevance_score']:.4f})\")\n",
        "\n",
        "# Print summaries (truncated for brevity)\n",
        "print(\"\\nConsolidated Summary:\")\n",
        "print(\"=================================\")\n",
        "for model, summary in results[\"summaries\"].items():\n",
        "    if summary:\n",
        "        print(f\"\\n{model.capitalize()} Summary: {summary[:]}...\")\n",
        "\n",
        "print(\"\\nSummaries for each paper:\")\n",
        "print(\"=================================\")\n",
        "\n",
        "# Print BERT Score evaluation\n",
        "if \"evaluation\" in results:\n",
        "    print(\"\\nBERT Score Evaluation:\")\n",
        "    print(\"=====================\")\n",
        "    for model, scores in results[\"evaluation\"].items():\n",
        "        if scores:\n",
        "            print(f\"\\n{model.capitalize()}:\")\n",
        "            print(f\"  Precision: {scores['precision']:.4f}\")\n",
        "            print(f\"  Recall: {scores['recall']:.4f}\")\n",
        "            print(f\"  F1: {scores['f1']:.4f}\")\n"
      ],
      "metadata": {
        "id": "5qGgP4xmlf5A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0528e57f-1d25-4611-93b9-57458ec3beb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error extracting abstract from PDF: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('punkt_tab')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n",
            "Error extracting abstract from PDF: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('punkt_tab')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n",
            "Error extracting abstract from PDF: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('punkt_tab')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n",
            "Error extracting abstract from PDF: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('punkt_tab')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n",
            "Error extracting abstract from PDF: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('punkt_tab')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n",
            "Error extracting abstract from PDF: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('punkt_tab')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n",
            "Error extracting abstract from PDF: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('punkt_tab')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n",
            "Error extracting abstract from PDF: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('punkt_tab')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n",
            "Getting model rankings...\n",
            "Sending prompt to Mistral API...\n",
            "Warning: Response is not valid JSON. Returning raw text.\n",
            "Creating ensemble ranking...\n",
            "Final ranking...\n",
            "Error extracting abstract from PDF: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('punkt_tab')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n",
            "Error extracting abstract from PDF: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('punkt_tab')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n",
            "Error extracting abstract from PDF: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('punkt_tab')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n",
            "Error extracting abstract from PDF: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('punkt_tab')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n",
            "Error extracting abstract from PDF: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('punkt_tab')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n",
            "Error extracting abstract from PDF: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('punkt_tab')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n",
            "Error extracting abstract from PDF: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('punkt_tab')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n",
            "Error extracting abstract from PDF: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('punkt_tab')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n",
            "Ranking 7 abstracts based on relevance to research question...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertModel were not initialized from the model checkpoint at naver/splade-cocondenser-ensembledistil and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating model summaries...\n",
            "Sending prompt to Mistral API...\n",
            "Warning: Response is not valid JSON. Returning raw text.\n",
            "Creating ensemble summary...\n",
            "Sending prompt to Mistral API...\n",
            "Warning: Response is not valid JSON. Returning raw text.\n",
            "Generating model summaries for each paper...\n",
            "Sending prompt to Mistral API...\n",
            "Warning: Response is not valid JSON. Returning raw text.\n",
            "Sending prompt to Mistral API...\n",
            "Warning: Response is not valid JSON. Returning raw text.\n",
            "Sending prompt to Mistral API...\n",
            "Warning: Response is not valid JSON. Returning raw text.\n",
            "Sending prompt to Mistral API...\n",
            "Warning: Response is not valid JSON. Returning raw text.\n",
            "Sending prompt to Mistral API...\n",
            "Warning: Response is not valid JSON. Returning raw text.\n",
            "Sending prompt to Mistral API...\n",
            "Warning: Response is not valid JSON. Returning raw text.\n",
            "Sending prompt to Mistral API...\n",
            "Warning: Response is not valid JSON. Returning raw text.\n",
            "Creating ensemble summary for each paper..\n",
            "Sending prompt to Mistral API...\n",
            "Warning: Response is not valid JSON. Returning raw text.\n",
            "Sending prompt to Mistral API...\n",
            "Warning: Response is not valid JSON. Returning raw text.\n",
            "Sending prompt to Mistral API...\n",
            "Warning: Response is not valid JSON. Returning raw text.\n",
            "Sending prompt to Mistral API...\n",
            "Warning: Response is not valid JSON. Returning raw text.\n",
            "Sending prompt to Mistral API...\n",
            "Warning: Response is not valid JSON. Returning raw text.\n",
            "Sending prompt to Mistral API...\n",
            "Warning: Response is not valid JSON. Returning raw text.\n",
            "Sending prompt to Mistral API...\n",
            "Warning: Response is not valid JSON. Returning raw text.\n",
            "Evaluating summaries with BERT Score...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Paper Rankings:\n",
            "==============\n",
            "\n",
            "Gemini Ranking:\n",
            "1. Lessons Learned from Natural Disasters around Digital Health Technologies and Delivering Quality Healthcare.\n",
            "2. Managing risk, governmentality and geoinformation: Vectors of vulnerability in the mapping of COVID‐19\n",
            "3. Mobile Phone Network Data in the COVID-19 era: A systematic review of applications, socioeconomic factors affecting compliance to non-pharmaceutical interventions, privacy implications, and post-pandemic economic recovery strategies.\n",
            "4. The critical elements of the health system that could make for resilience in the World Health Organization African Region: a scoping review.\n",
            "5. A scoping review of wildfire smoke risk communications: issues, gaps, and recommendations.\n",
            "6. A Topical Review on Enabling Technologies for the Internet of Medical Things: Sensors, Devices, Platforms, and Applications.\n",
            "7. New York State Climate Impacts Assessment Chapter 03: Agriculture.\n",
            "\n",
            "Mistral Ranking:\n",
            "1. Lessons Learned from Natural Disasters around Digital Health Technologies and Delivering Quality Healthcare.\n",
            "2. Managing risk, governmentality and geoinformation: Vectors of vulnerability in the mapping of COVID‐19\n",
            "3. The critical elements of the health system that could make for resilience in the World Health Organization African Region: a scoping review.\n",
            "4. A scoping review of wildfire smoke risk communications: issues, gaps, and recommendations.\n",
            "5. Mobile Phone Network Data in the COVID-19 era: A systematic review of applications, socioeconomic factors affecting compliance to non-pharmaceutical interventions, privacy implications, and post-pandemic economic recovery strategies.\n",
            "6. A Topical Review on Enabling Technologies for the Internet of Medical Things: Sensors, Devices, Platforms, and Applications.\n",
            "7. New York State Climate Impacts Assessment Chapter 03: Agriculture.\n",
            "\n",
            "Ensemble Ranking:\n",
            "1. Lessons Learned from Natural Disasters around Digital Health Technologies and Delivering Quality Healthcare. (Score: 0.1250)\n",
            "2. Managing risk, governmentality and geoinformation: Vectors of vulnerability in the mapping of COVID‐19 (Score: 0.1429)\n",
            "3. A scoping review of wildfire smoke risk communications: issues, gaps, and recommendations. (Score: 0.1667)\n",
            "4. The critical elements of the health system that could make for resilience in the World Health Organization African Region: a scoping review. (Score: 0.2500)\n",
            "5. Mobile Phone Network Data in the COVID-19 era: A systematic review of applications, socioeconomic factors affecting compliance to non-pharmaceutical interventions, privacy implications, and post-pandemic economic recovery strategies. (Score: 0.2222)\n",
            "6. A Topical Review on Enabling Technologies for the Internet of Medical Things: Sensors, Devices, Platforms, and Applications. (Score: 0.2857)\n",
            "7. New York State Climate Impacts Assessment Chapter 03: Agriculture. (Score: 0.5000)\n",
            "\n",
            "Final Ranking:\n",
            "1.Lessons Learned from Natural Disasters around Digital Health Technologies and Delivering Quality Healthcare. (Score: 0.0651)\n",
            "2.Managing risk, governmentality and geoinformation: Vectors of vulnerability in the mapping of COVID‐19 (Score: 0.0640)\n",
            "3.A scoping review of wildfire smoke risk communications: issues, gaps, and recommendations. (Score: 0.0627)\n",
            "4.A Topical Review on Enabling Technologies for the Internet of Medical Things: Sensors, Devices, Platforms, and Applications. (Score: 0.0623)\n",
            "5.Mobile Phone Network Data in the COVID-19 era: A systematic review of applications, socioeconomic factors affecting compliance to non-pharmaceutical interventions, privacy implications, and post-pandemic economic recovery strategies. (Score: 0.0620)\n",
            "6.The critical elements of the health system that could make for resilience in the World Health Organization African Region: a scoping review. (Score: 0.0620)\n",
            "7.New York State Climate Impacts Assessment Chapter 03: Agriculture. (Score: 0.0597)\n",
            "\n",
            "Consolidated Summary:\n",
            "=================================\n",
            "\n",
            "Gemini Summary: **Summary of Mobile Apps in Natural Disaster Mental Health Response Strategies**\n",
            "\n",
            "**1. Key Themes and Findings:**\n",
            "\n",
            "*   **Digital Health Technologies & Resilience:** Explores the potential and limitations of digital health in natural disasters, emphasizing the need for climate-resilient solutions to deliver healthcare.\n",
            "*   **Geoinformation & Governmentality:** Examines the use of geoinformation (including mobile data) during COVID-19 for monitoring and controlling populations, raising ethical concerns about privacy and trust.\n",
            "*   **Risk Communication Gaps:** Highlights gaps in wildfire smoke risk communication research, especially for vulnerable populations. Focuses on readability, accessibility, and actionability\n",
            "*   **Health System Resilience:** Identifies key elements for health system resilience in the WHO African Region during crises, including service delivery, workforce strengthening, community engagement, and adaptable policies.\n",
            "*   **Mobile Phone Network Data (MPND) & COVID-19:** Reviews the use of MPND for tracking mobility, assessing NPI compliance, and analyzing economic recovery during COVID-19, emphasizing privacy concerns.\n",
            "\n",
            "**2. Methodological Approaches:**\n",
            "\n",
            "*   **Mixed-Methods Reviews:** Used to map knowledge about digital health resilience in disasters (Paper 1).\n",
            "*   **Scoping Reviews:** Employed to identify gaps in wildfire smoke risk communications research and to map health system resilience strategies (Papers 3 & 4).\n",
            "*   **Systematic Reviews:** Conducted to analyze MPND applications during COVID-19, using PRISMA guidelines and quality appraisal tools (MMAT, CASP) (Paper 5).\n",
            "*   **Qualitative Thematic Analysis:** Used to analyze communication materials and messaging recommendations related to wildfire smoke (Paper 3).\n",
            "*   **Bibliometric Analysis:** Applied to visualize publication structures and keyword networks in MPND research (Paper 5).\n",
            "\n",
            "**3. Gaps in the Literature:**\n",
            "\n",
            "*   **Digital Health Resilience:** Limited evidence on the resilience and effectiveness of digital health in natural disasters.\n",
            "*   **Wildfire Smoke Communication:** Insufficient studies on behavior change, effective communication strategies, and risk communication for vulnerable populations.\n",
            "*   **Health System Resilience:** Need for more research on the efficacy of initiatives to improve health system resilience, particularly in vulnerable African health systems.\n",
            "*   **Ethical Use of Geoinformation:** Lack of clarity on balancing the use of geoinformation for public health with ethical considerations.\n",
            "*   **Privacy Concerns with MPND:** Need for further exploration of ethical requirements and mitigation of privacy risks related to MPND usage.\n",
            "\n",
            "**4. Directions for Future Research:**\n",
            "\n",
            "*   **Developing Climate-Resilient Digital Health Interventions:** Focus on building digital health solutions that can withstand natural disasters.\n",
            "*   **Improving Wildfire Smoke Risk Communication:** Develop and evaluate communication strategies tailored to specific needs of at-risk groups, using mixed media and diverse dissemination methods.\n",
            "*   **Strengthening Health System Resilience:** Investigate strategies to improve health system resilience in vulnerable regions, including primary care and community engagement.\n",
            "*   **Addressing Ethical Implications of Geoinformation:** Develop ethical guidelines for the use of geoinformation in public health emergencies.\n",
            "*   **Balancing MPND Utility with Privacy:** Explore methods for using MPND while protecting individual privacy and addressing ethical concerns.\n",
            "...\n",
            "\n",
            "Mistral Summary: ### Comprehensive Summary\n",
            "\n",
            "#### Key Themes and Findings Across the Papers\n",
            "\n",
            "1. **Digital Health Technologies and Resilience**:\n",
            "   - Digital health technologies are crucial for delivering healthcare in non-optimal conditions, especially during natural disasters and pandemics.\n",
            "   - These technologies improve access, reduce inefficiencies, and enhance the portability of patient information.\n",
            "   - The resilience and effectiveness of digital health technologies in natural disasters need further evaluation.\n",
            "\n",
            "2. **Geoinformation and Surveillance**:\n",
            "   - Geoinformation, including satellite data and contact tracing apps, has been instrumental in monitoring and controlling the spread of COVID-19.\n",
            "   - Ethical concerns arise from the use of geoinformation for surveillance, impacting trust, solidarity, and citizen rights.\n",
            "\n",
            "3. **Wildfire Smoke Risk Communications**:\n",
            "   - Effective communication strategies for wildfire smoke exposure are limited, with a need for more health literacy principles.\n",
            "   - Communications should be easy-to-understand, actionable, and tailored to at-risk populations.\n",
            "   - Mixed media formats and diverse dissemination strategies are recommended.\n",
            "\n",
            "4. **Health System Resilience**:\n",
            "   - Health systems need to be resilient to cope with emergencies and foster continuous improvement.\n",
            "   - Key strategies include continuous delivery of essential services, strengthening the health workforce, community engagement, and flexible governance.\n",
            "\n",
            "5. **Mobile Phone Network Data (MPND)**:\n",
            "   - MPND has been pivotal in monitoring and controlling the spread of COVID-19.\n",
            "   - Socioeconomic factors significantly affect compliance with non-pharmaceutical interventions (NPIs).\n",
            "   - Privacy and ethical concerns are raised by the use of MPND, necessitating a balance between public health needs and privacy.\n",
            "\n",
            "#### Methodological Approaches Used\n",
            "\n",
            "1. **Mixed-Methods Review**:\n",
            "   - Used in the first paper to map digital health resilience in natural disasters, incorporating case studies.\n",
            "\n",
            "2. **Scoping Reviews**:\n",
            "   - Employed in the third and fourth papers to identify gaps in wildfire smoke risk communications and health system resilience in the WHO African Region.\n",
            "\n",
            "3. **Systematic Review**:\n",
            "   - Applied in the fifth paper to evaluate the use of MPND during the COVID-19 pandemic, following PRISMA guidelines and using MMAT and CASP for assessment.\n",
            "\n",
            "4. **Bibliometric Analysis**:\n",
            "   - Used in the fifth paper to visualize publication structures, co-authorship networks, and keyword co-occurrence networks.\n",
            "\n",
            "#### Gaps in the Literature\n",
            "\n",
            "1. **Effectiveness of Digital Health Technologies**:\n",
            "   - Limited research on the resilience and effectiveness of digital health technologies in natural disasters.\n",
            "\n",
            "2. **Risk Communications for Vulnerable Populations**:\n",
            "   - Insufficient studies on risk communications for vulnerable populations, especially in the context of wildfire smoke exposure.\n",
            "\n",
            "3. **Health System Resilience**:\n",
            "   - More research is needed to assess the efficacy of initiatives for improving health system resilience, particularly in vulnerable regions.\n",
            "\n",
            "4. **Ethical and Privacy Concerns**:\n",
            "   - Need for further investigation into the ethical and privacy implications of using geoinformation and MPND.\n",
            "\n",
            "#### Directions for Future Research\n",
            "\n",
            "1. **Intervention Research**:\n",
            "   - More intervention research and effectiveness evaluations of risk communications about wildfire smoke exposure.\n",
            "\n",
            "2. **Health System Resilience**:\n",
            "   - Further studies on the efficacy of strategies to improve health system resilience, especially in vulnerable regions.\n",
            "\n",
            "3. **Ethical and Privacy Considerations**:\n",
            "   - Research on balancing public health needs with privacy and ethical concerns in the use of geoinformation and MPND.\n",
            "\n",
            "4. **Tailored Communications**:\n",
            "   - Development and dissemination of risk communications tailored to the specific needs of at-risk groups....\n",
            "\n",
            "Ensemble Summary: ### Synthesis of Mobile App Use in Disaster Mental Health Response Strategies\n",
            "\n",
            "This review maps the current landscape of research on the use of mobile apps and related technologies within natural disaster mental health response strategies. Key themes emerging from the literature include the role of digital health technologies in building resilience, the ethical considerations surrounding geoinformation and surveillance, the importance of effective risk communication, the components of health system resilience, and the applications of mobile phone network data (MPND).\n",
            "\n",
            "Digital health technologies, including mobile apps, are viewed as potentially crucial for delivering healthcare services during and after natural disasters and pandemics. These technologies aim to improve access, reduce inefficiencies, and enhance portability of patient information. However, the resilience and actual effectiveness of these tools in disaster contexts remain significantly under-researched.\n",
            "\n",
            "The use of geoinformation technologies, such as satellites, drones, and contact tracing apps, has expanded during crises like the COVID-19 pandemic, providing data for capacity building and informing policy decisions. This expansion brings ethical considerations related to surveillance, data privacy, and the potential infringement on citizen rights.\n",
            "\n",
            "Effective risk communication is highlighted as essential for managing public health crises, particularly concerning issues like wildfire smoke exposure. Best practices emphasize readability, accessibility, actionability, and tailoring messages to vulnerable populations. A critical gap is the limited number of studies evaluating the effectiveness of different risk communication strategies and interventions.\n",
            "\n",
            "Health system resilience is characterized by the continuous delivery of essential services, a strong and supported health workforce, active community engagement, and adaptable governance structures, along with post-recovery planning. More research is needed to assess the efficacy of initiatives designed to improve health system resilience, especially within vulnerable health systems and regions like those in Africa.\n",
            "\n",
            "MPND has been used extensively to monitor and control the spread of diseases like COVID-19 by tracking human mobility patterns. Socioeconomic factors influence compliance with public health interventions. However, the use of MPND raises significant privacy and ethical concerns that require further investigation and careful consideration.\n",
            "\n",
            "**Methodological Approaches:** Studies in this area have utilized mixed-methods reviews to map digital health resilience, scoping reviews to identify gaps in communication strategies and health system resilience, systematic reviews (following PRISMA guidelines) to analyze MPND applications, and critical geography/surveillance studies to examine the ethical implications of geoinformation.\n",
            "\n",
            "**Gaps in the Literature:** Key gaps include: limited research on the effectiveness and resilience of digital health technologies in disaster settings; a lack of studies evaluating risk communications tailored to vulnerable populations; insufficient research on initiatives improving health system resilience, particularly in vulnerable regions; and the need for investigation into balancing ethical requirements and privacy risks associated with using MPND and geoinformation technologies.\n",
            "\n",
            "**Directions for Future Research:** Priorities include: conducting intervention research to evaluate the effectiveness of digital health tools and risk communications; exploring the ethical and privacy implications of using geoinformation and MPND in public health responses; developing and disseminating risk communications tailored to the specific needs of at-risk groups; and assessing the efficacy of initiatives for improving health system resilience, with a focus on continuous improvement strategies and vulnerable regions.\n",
            "...\n",
            "\n",
            "Gemini_ep Summary: **Summary of New York State Climate Impacts Assessment Chapter 03: Agriculture:**\n",
            "\n",
            "*   **Importance:** Agriculture is a key New York State industry, particularly for dairy, fruits, and other commodities.\n",
            "\n",
            "*   **Climate Change Impacts:** The agricultural sector faces significant challenges due to its dependence on weather and specific climatic conditions amidst a changing climate.\n",
            "\n",
            "*   **Focus:** The chapter examines the various impacts of climate change on agriculture in New York.\n",
            "\n",
            "*   **Scope:** It analyzes the interaction of these impacts with existing challenges for farmers and farmworkers.\n",
            "\n",
            "*   **Solutions:** The chapter explores adaptation strategies and opportunities for building resilience within the agriculture industry.\n",
            "**Summary:**\n",
            "\n",
            "*   **Focus:** Explores the application of the Internet of Things (IoT) in the medical sector (IoMT).\n",
            "*   **Motivation:** Notes that IoMT is lagging compared to other industries despite its potential.\n",
            "*   **Methodology:** Systematic review following PRISMA guidelines, analyzing research articles and websites related to IoMT sensors and devices. Initial search yielded 986 articles, refined to 597, and then focused on 23 relevant studies and website records.\n",
            "*   **Scope:** Examines medical IoT sensors and devices, IoT platforms for data visualization, and the role of artificial intelligence in medical applications. Includes detailed analysis of sensor monitoring circuits in an ICU setting, device applications, and data management systems.\n",
            "*   **Key Areas:** Covers sensor monitoring circuits, device applications, and data management system\n",
            "*   **Outcomes:** Identifies challenges and prospects for IoMT implementation.\n",
            "This systematic review examines the application of mobile phone network data (MPND) in analyzing the COVID-19 pandemic, its impact, and subsequent recovery. The review, following PRISMA guidelines, analyzes 55 relevant studies. These studies are classified into five main areas: monitoring human mobility, correlating mobility with COVID-19 spread, analyzing economic and travel recovery, assessing factors affecting compliance with non-pharmaceutical interventions (NPIs), and investigating the impact of lockdowns and NPIs on human behavior and economic activity. The review highlights the significant impact of NPIs on reducing mobility but also notes that demographics, political affiliation, socioeconomic inequalities, and racial inequalities affected adherence to NPIs. The conclusion emphasizes the privacy implications of using MPND and calls for further investigation into balancing ethical requirements with privacy risks.\n",
            "This scoping review examined research on wildfire smoke risk communications to identify gaps and inform best practices. The review analyzed 21 peer-reviewed articles from multiple countries, revealing limitations in studies describing behavior change, effective communication materials, delivery strategies, and risk communications for vulnerable populations. Key findings include a need for easy-to-understand communications tailored to at-risk groups, concise messaging with specific actions, mixed media formats, and diverse dissemination strategies. The review concludes there is a pressing need for intervention research, effectiveness evaluation, and the development/dissemination of risk communications for both the general public and vulnerable populations.\n",
            "**Objective:** Identify health system components needed to foster resilience in the WHO African Region during health emergencies.\n",
            "\n",
            "**Methods:** Scoping review of Scopus, PubMed, and grey literature using JBI methodology. Key findings on health system resilience were mapped to WHO's health system components.\n",
            "\n",
            "**Results:** 28 studies highlighted strategies like continuous essential service delivery, strengthened health workforce (including community workers), community engagement, workforce protection, and flexible leadership/governance.\n",
            "\n",
            "**Conclusion:** Resilient health systems require comprehensive strategies across all healthcare delivery levels, including primary care, adaptable policies for crisis response and recovery, and continuous improvement. More research is needed to assess initiative effectiveness, particularly in vulnerable African health systems.\n",
            "The research paper examines the use of geoinformation technologies (satellites, drones, online dashboards, contact tracing apps) by governments worldwide to monitor and control the spread of COVID-19. It highlights geoinformation's dual role: supporting capacity building and mapping vulnerable communities in deprived urban areas, while also enabling government control over citizen behavior. The paper argues that this geoinformation-driven governance, viewed through critical geography and surveillance studies, potentially endangers ethical values, transparency, and citizens' rights.\n",
            "This research paper reviews the resilience and effectiveness of digital health technologies during natural disasters, aiming to identify lessons learned and propose future directions for building climate-resilient digital health interventions. It employs a mixed-methods review methodology, using case studies to examine the successes and failures of digital health in disaster response. The goal is to inform the development of healthcare systems that can deliver safe, quality care, especially in remote and underserved areas, despite the increasing frequency and severity of weather-related events due to climate change. The paper acknowledges digital health's potential for improved access, efficiency, and portability of information but seeks to understand its limitations in crisis situations.\n",
            "...\n",
            "\n",
            "Mistral_ep Summary: Agriculture is a crucial industry in New York State, known for its high production of dairy, fruits, and various other commodities. The sector faces significant challenges due to climate change, as it heavily relies on specific weather and climatic conditions. This chapter of the New York State Climate Impacts Assessment examines the multifaceted impacts of climate change on agriculture, including how these impacts intersect with other issues faced by farmers and farmworkers. It also explores potential opportunities for the agriculture industry to adapt and enhance its resilience to these changes.**Summary:**\n",
            "\n",
            "- **Focus Area:** The paper reviews enabling technologies for the Internet of Medical Things (IoMT), focusing on sensors, devices, platforms, and applications in the medical sector.\n",
            "\n",
            "- **Background:** The Internet of Things (IoT) has significant potential in healthcare but lags behind other sectors in medical applications.\n",
            "\n",
            "- **Methodology:** A systematic review following PRISMA guidelines was conducted on research articles and websites related to IoMT sensors and devices. Initially, 986 articles were selected post-2001, narrowed down to 597 through inclusion-exclusion criteria, with 23 new studies identified.\n",
            "\n",
            "- **Key Topics:**\n",
            "  - **IoMT Sensors and Devices:** Detailed analysis of sensor monitoring circuits, particularly in Intensive Care Unit (ICU) scenarios.\n",
            "  - **IoT Platforms:** Data visualization and management systems for patient monitoring.\n",
            "  - **Artificial Intelligence:** Applications in medical fields.\n",
            "\n",
            "- **Findings:** The review discusses various device applications, data management systems, and IoT platforms for patient monitoring.\n",
            "\n",
            "- **Conclusion:** The paper outlines detailed discussions on challenges and presents potential prospects for the future of IoMT.**Summary:**\n",
            "\n",
            "- **Background**: Traditional mobility datasets are limited in capturing real-time human mobility patterns. Mobile Phone Network Data (MPND) has been crucial in monitoring and controlling COVID-19 due to the widespread use of smartphones and telecommunication networks.\n",
            "\n",
            "- **Methods**: The study conducted a systematic review following PRISMA guidelines, evaluated studies using MMAT and CASP, and applied bibliometric analysis to visualize publication structures and networks.\n",
            "\n",
            "- **Results**: 55 studies were identified, with 46 quantitative and 9 qualitative. These studies were categorized into five groups:\n",
            "  1. Monitoring and tracking human mobility patterns (11 studies).\n",
            "  2. Investigating the correlation between mobility patterns and COVID-19 spread (7 studies).\n",
            "  3. Analyzing the recovery of economic activities and travel patterns (5 studies).\n",
            "  4. Assessing factors associated with compliance to Non-Pharmaceutical Interventions (NPI) (5 studies).\n",
            "  5. Investigating the impact of COVID-19 lockdowns and NPI measures (18 studies).\n",
            "\n",
            "- **Findings**: NPI measures significantly reduced human movement. However, adherence to these measures was influenced by demographics, political affiliation, socioeconomic inequality, and racial inequality, affecting disease spread and recovery.\n",
            "\n",
            "- **Conclusion**: The use of MPND for monitoring human activities during COVID-19 raises privacy and ethical concerns, necessitating further investigation to balance ethical requirements and privacy risks.**Summary:**\n",
            "\n",
            "- **Background**: Wildfire smoke exposure is a growing public health concern due to increasing incidence and severity of megafires. Effective communication to reduce public exposure is crucial but often lacks adherence to health literacy principles.\n",
            "\n",
            "- **Objective**: To identify gaps in research and evaluation of wildfire smoke risk communications and programs.\n",
            "\n",
            "- **Methods**: A scoping review of 21 peer-reviewed studies from Web of Science and PubMed databases, focusing on communication materials, messaging, delivery strategies, behavior change, and vulnerable populations.\n",
            "\n",
            "- **Geographical Focus**: Studies based in the US, Australia, Canada, Italy, and other countries.\n",
            "\n",
            "- **Findings**:\n",
            "  - Limited studies on behavior change to reduce wildfire smoke exposure.\n",
            "  - Insufficient research on effective communication materials, messaging, and delivery strategies.\n",
            "  - Even fewer studies on risk communications for vulnerable populations.\n",
            "\n",
            "- **Recommendations**:\n",
            "  - Develop easy-to-understand communications tailored to at-risk groups.\n",
            "  - Provide specific actionable messages for avoiding smoke exposure.\n",
            "  - Utilize mixed media formats and diverse dissemination strategies.\n",
            "  - Conduct more intervention research and effectiveness evaluations.\n",
            "  - Increase development and dissemination of risk communications for both the general public and vulnerable populations.**Summary:**\n",
            "\n",
            "**Purpose:**\n",
            "The scoping review aimed to identify the critical elements and components of health systems necessary for resilience in the World Health Organization (WHO) African Region, particularly in the face of unpredictable events like infectious disease outbreaks and humanitarian crises.\n",
            "\n",
            "**Methods:**\n",
            "The review involved a systematic search of Scopus, PubMed, and grey literature, following the Joanna Briggs Institute methodology. Studies were selected based on predefined eligibility criteria, and findings were mapped according to the WHO’s core health system components.\n",
            "\n",
            "**Key Findings:**\n",
            "- **Continuous Delivery of Essential Services:** Ensuring the uninterrupted provision of essential healthcare services.\n",
            "- **Strengthening the Health Workforce:** Emphasis on training and supporting healthcare workers, including community health workers.\n",
            "- **Community Engagement:** Involving the community in health initiatives to enhance responsiveness and adaptability.\n",
            "- **Protective Mechanisms for Health Workforce:** Implementing measures to safeguard the health and well-being of healthcare workers.\n",
            "- **Flexible Leadership and Governance:** Adopting adaptable leadership and governance structures to manage crises effectively.\n",
            "- **Primary Care Focus:** Highlighting the importance of primary care in building a resilient health system.\n",
            "\n",
            "**Conclusion:**\n",
            "A resilient health system in the WHO African Region should be prepared for crises, have adaptable policies, and plan for post-recovery. Continuous improvement and more research are needed to evaluate the effectiveness of resilience-building initiatives, especially in vulnerable African health systems.The research paper \"Managing risk, governmentality and geoinformation: Vectors of vulnerability in the mapping of COVID‐19\" examines the use of geoinformation technologies during the COVID-19 pandemic. Governments worldwide employed geoinformation from satellites, drones, online dashboards, and contact tracing apps to monitor and control the virus's spread. This geoinformation was crucial in two main ways:\n",
            "\n",
            "1. **Capacity Building and Urban Policy**: Geoinformation aided in mapping deprived urban areas (slums) and their responses to COVID-19 measures. Initiatives by the United Nations and NGOs used this data to inform urban policymaking, addressing social, political, and environmental issues faced by residents in these areas.\n",
            "\n",
            "2. **Surveillance and Control**: Geoinformation was also used to monitor and limit citizens' behavior, aiming to control the pandemic's spread. However, this form of geoinformation-driven governmentality raises ethical concerns. From a critical geography and surveillance studies perspective, it endangers values such as trust, solidarity, agency, transparency, and citizens' rights.**Summary:**\n",
            "\n",
            "- **Context**: Climate change is intensifying natural disasters, necessitating climate-resilient healthcare systems, particularly in remote or underserved areas.\n",
            "- **Role of Digital Health Technologies**: These technologies can enhance healthcare delivery by improving access, reducing inefficiencies and costs, and increasing the portability of patient information. They also support personalized healthcare and patient engagement.\n",
            "- **COVID-19 Pandemic**: Digital health technologies were rapidly deployed during the pandemic to comply with public health measures, demonstrating their potential in crisis situations.\n",
            "- **Research Gap**: The resilience and effectiveness of digital health technologies in the face of natural disasters are not yet fully understood.\n",
            "- **Study Objective**: This review aims to map existing knowledge on digital health resilience in natural disasters using case studies to identify successful and unsuccessful strategies and propose future directions for building climate-resilient digital health interventions....\n",
            "\n",
            "Ensemble_ep Summary: Several studies address the increasing impact of environmental crises on various sectors and populations, examining vulnerabilities, technological responses, and ethical considerations. Research highlights the susceptibility of agriculture, particularly in regions like New York, to climate change, emphasizing the need for adaptation strategies and resilience-building. This vulnerability extends to public health, where wildfire smoke exposure poses a growing risk, demanding effective risk communication tailored to at-risk groups.\n",
            "\n",
            "In response to such crises, digital technologies are playing an increasingly prominent role. The Internet of Medical Things (IoMT) offers potential advancements in healthcare through sensors, devices, and data management platforms, though challenges and limitations remain in their implementation. Mobile Phone Network Data (MPND) emerged as a valuable tool during the COVID-19 pandemic for monitoring mobility and assessing the effectiveness of Non-Pharmaceutical Interventions (NPIs), but its use raises critical privacy and ethical concerns. Geoinformation technologies, including satellite and drone imagery, aided in mapping vulnerable communities and managing the pandemic, but also present the risk of undermining citizen rights and transparency through government surveillance.\n",
            "\n",
            "Building resilience within health systems, particularly in the WHO African Region, requires continuous essential service delivery, a strengthened health workforce (including community health workers), community engagement, flexible leadership and governance, and a focus on primary care. Digital health technologies offer the potential to enhance climate change adaptation and mitigation within healthcare by improving access, reducing inefficiencies, and increasing patient engagement. However, further research is needed to evaluate the effectiveness of resilience-building initiatives and climate-resilient digital health interventions, especially in vulnerable and underserved populations. Ultimately, balancing technological advancements with ethical considerations and addressing the specific needs of vulnerable groups are crucial for effective disaster response strategies.\n",
            "...\n",
            "\n",
            "Summaries for each paper:\n",
            "=================================\n",
            "\n",
            "BERT Score Evaluation:\n",
            "=====================\n",
            "\n",
            "Gemini:\n",
            "  Precision: -0.2043\n",
            "  Recall: -0.0251\n",
            "  F1: -0.1145\n",
            "\n",
            "Mistral:\n",
            "  Precision: -0.1724\n",
            "  Recall: -0.0517\n",
            "  F1: -0.1110\n",
            "\n",
            "Ensemble:\n",
            "  Precision: 0.0017\n",
            "  Recall: 0.0185\n",
            "  F1: 0.0117\n",
            "\n",
            "Gemini_ep:\n",
            "  Precision: -0.1677\n",
            "  Recall: -0.0541\n",
            "  F1: -0.1098\n",
            "\n",
            "Mistral_ep:\n",
            "  Precision: -0.1239\n",
            "  Recall: -0.0579\n",
            "  F1: -0.0893\n",
            "\n",
            "Ensemble_ep:\n",
            "  Precision: -0.0347\n",
            "  Recall: -0.0905\n",
            "  F1: -0.0610\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reference_summary = \"\"\"\n",
        "# Background\n",
        "# Disasters are becoming more frequent due to the impact of extreme weather events attributed to climate change, causing loss of lives, property, and psychological trauma. Mental health response to disasters emphasizes prevention and mitigation, and mobile health (mHealth) apps have been used for mental health promotion and treatment. However, little is known about their use in the mental health components of disaster management.\n",
        "\n",
        "# Objective\n",
        "# This scoping review was conducted to explore the use of mobile phone apps for mental health responses to natural disasters and to identify gaps in the literature.\n",
        "\n",
        "# Methods\n",
        "# We identified relevant keywords and subject headings and conducted comprehensive searches in 6 electronic databases. Studies in which participants were exposed to a man-made disaster were included if the sample also included some participants exposed to a natural hazard. Only full-text studies published in English were included. The initial titles and abstracts of the unique papers were screened by 2 independent review authors. Full texts of the selected papers that met the inclusion criteria were reviewed by the 2 independent reviewers. Data were extracted from each selected full-text paper and synthesized using a narrative approach based on the outcome measures, duration, frequency of use of the mobile phone apps, and the outcomes. This scoping review was reported according to the PRISMA-ScR (Preferred Reporting Items for Systematic Reviews and Meta-Analyses extension for Scoping Reviews).\n",
        "\n",
        "# Results\n",
        "# Of the 1398 papers retrieved, 5 were included in this review. A total of 3 studies were conducted on participants exposed to psychological stress following a disaster while 2 were for disaster relief workers. The mobile phone apps for the interventions included Training for Life Skills, Sonoma Rises, Headspace, Psychological First Aid, and Substance Abuse and Mental Health Services Administration (SAMHSA) Behavioural Health Disaster Response Apps. The different studies assessed the effectiveness or efficacy of the mobile app, feasibility, acceptability, and characteristics of app use or predictors of use. Different measures were used to assess the effectiveness of the apps’ use as either the primary or secondary outcome.\n",
        "\n",
        "# Conclusions\n",
        "# A limited number of studies are exploring the use of mobile phone apps for mental health responses to disasters. The 5 studies included in this review showed promising results. Mobile apps have the potential to provide effective mental health support before, during, and after disasters. However, further research is needed to explore the potential of mobile phone apps in mental health responses to all hazards.\n",
        "\n",
        "# Keywords: mental health, disasters, mobile health, mHealth, application, applications, app, apps, smartphone, stress, psychological, traumatic, disaster, disasters, hazard, hazards, emergency, psychological trauma, mobile apps, trauma, scoping, review methods, review methodology, mobile phone\n",
        "# \"\"\"\n",
        "# abstracts, titles = get_abstracts_from_papers(search_results)\n",
        "# results = run_review_pipeline(query, abstracts, titles, reference_summary)\n",
        "\n",
        "# # Print rankings\n",
        "# print(\"\\nPaper Rankings:\")\n",
        "# print(\"==============\")\n",
        "# print(\"\\nGemini Ranking:\")\n",
        "# # print(\"results!!!----------->\", results)\n",
        "# for i, idx in enumerate(results[\"rankings\"][\"gemini\"][\"ranking\"]):\n",
        "#     print(f\"{i+1}. {titles[idx-1]}\")\n",
        "\n",
        "# print(\"\\nMistral Ranking:\")\n",
        "# for i, idx in enumerate(results[\"rankings\"][\"mistral\"][\"ranking\"]):\n",
        "#     print(f\"{i+1}. {titles[idx-1]}\")\n",
        "\n",
        "# print(\"\\nEnsemble Ranking:\")\n",
        "# for i, idx in enumerate(results[\"rankings\"][\"ensemble\"][\"ranking\"]):\n",
        "#     print(f\"{i+1}. {titles[idx]} (Score: {results['rankings']['ensemble']['scores'][i]:.4f})\")\n",
        "\n",
        "# print(\"\\nFinal Ranking:\")\n",
        "# for i in (results[\"rankings\"][\"final\"]):\n",
        "#   print(f\"{i['rank']}.{i['title']} (Score: {i['relevance_score']:.4f})\")\n",
        "\n",
        "# # Print summaries (truncated for brevity)\n",
        "# print(\"\\nConsolidated Summary:\")\n",
        "# print(\"=================================\")\n",
        "# for model, summary in results[\"summaries\"].items():\n",
        "#     if summary:\n",
        "#         print(f\"\\n{model.capitalize()} Summary: {summary[:]}...\")\n",
        "\n",
        "# # Print BERT Score evaluation\n",
        "# if \"evaluation\" in results:\n",
        "#     print(\"\\nBERT Score Evaluation:\")\n",
        "#     print(\"=====================\")\n",
        "#     for model, scores in results[\"evaluation\"].items():\n",
        "#         if scores:\n",
        "#             print(f\"\\n{model.capitalize()}:\")\n",
        "#             print(f\"  Precision: {scores['precision']:.4f}\")\n",
        "#             print(f\"  Recall: {scores['recall']:.4f}\")\n",
        "#             print(f\"  F1: {scores['f1']:.4f}\")\n"
      ],
      "metadata": {
        "id": "DEL_VI6N7rqp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}